{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import glob\n",
    "import os \n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xC-87z2GWMq",
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nlp-course' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kfirbar/nlp-course.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls nlp-course/lm-languages-data-new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'en_df': 'en.csv',\n",
    "              'es_df': 'es.csv',\n",
    "              'fr_df': 'fr.csv',\n",
    "              'in_df': 'in.csv',\n",
    "              'it_df': 'it.csv',\n",
    "              'nl_df': 'nl.csv',\n",
    "              'pt_df': 'pt.csv',\n",
    "              'tl_df': 'tl.csv'}\n",
    "\n",
    "    \n",
    "directory = 'nlp-course/lm-languages-data-new/'    \n",
    "for (key, value) in data_files.items():\n",
    "    data_files[key] = directory + value\n",
    "    \n",
    "languages_list = list(data_files.keys())\n",
    "start_token = '↠'\n",
    "end_token = '↞'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xCfzsITW8Yaj"
   },
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\"\n",
    "    data frame is table from 2 columns:\n",
    "        1. tweet id\n",
    "        2. tweet text\n",
    "    \"\"\"  \n",
    "    tokens = []\n",
    "    for path in data_files.values():\n",
    "        df = pd.read_csv(path)\n",
    "        for text in df['tweet_text'].values:\n",
    "            tokens.extend(list(text))\n",
    "    return list(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def tweets_to_text(data_file_path, n):\n",
    "    \"\"\"\n",
    "    data frame is table from 2 columns:\n",
    "        1. tweet id\n",
    "        2. tweet text\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(r''+ data_file_path)\n",
    "    # debug = True\n",
    "    # if debug == True:\n",
    "    #     df = df[0:100]\n",
    "    columns_list = df.columns.to_list()\n",
    "    tweets_list = df[columns_list[-1]].apply(lambda x: start_token + x + end_token).values\n",
    "    text = ''.join(tweets_list)\n",
    "    \n",
    "    text = start_token * (n-1) + text + end_token * (n-1)\n",
    "\n",
    "    return text\n",
    "\n",
    "def reorder_list(List, index_list):\n",
    "    return [List[i] for i in index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kMC_u8eQbVvZ"
   },
   "outputs": [],
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
    "    # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
    "    # data_file_path - the data_file from which we record probabilities for our model\n",
    "    # add_one - True/False (use add_one smoothing or not)\n",
    "  \n",
    "    lm_dict = {}\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    text = tweets_to_text(data_file_path, n)\n",
    "\n",
    "    # Extract n length substrings\n",
    "    n_gram = [text[i: i + n] for i in range(len(text) - n)]\n",
    "\n",
    "    lm_dict = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for i_n_gram in n_gram:\n",
    "        n_1_gram = i_n_gram[0:n-1]\n",
    "        lm_dict[n_1_gram][i_n_gram[n-1]] += 1\n",
    "    \n",
    "    for key in lm_dict.keys():\n",
    "        key_count = sum(lm_dict[key].values())\n",
    "        inner_dict = {}\n",
    "        for key_1 in lm_dict[key].keys():\n",
    "            if add_one:\n",
    "                inner_dict[key_1] = (lm_dict[key][key_1] + 1) / (key_count + V)\n",
    "            else:\n",
    "                inner_dict[key_1] = lm_dict[key][key_1]/ key_count\n",
    "        if add_one:\n",
    "            lm_dict[key] = defaultdict(lambda: 1 / (key_count + V), inner_dict)\n",
    "        else:\n",
    "            lm_dict[key] = defaultdict(lambda: 0, inner_dict)\n",
    "            \n",
    "    return lm_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F0kkMn328-lJ"
   },
   "outputs": [],
   "source": [
    "def eval(n, model, data_file):\n",
    "    # n - the n-gram that you used to build your model (must be the same number)\n",
    "    # model - the dictionary (model) to use for calculating perplexity\n",
    "    # data_file - the tweets file that you wish to claculate a perplexity score for\n",
    "\n",
    "    # read file\n",
    "    if os.path.exists(data_file):\n",
    "        text = tweets_to_text(data_file, n)\n",
    "    else:\n",
    "        text = data_file\n",
    "    # Extract n length substrings\n",
    "    n_gram = [text[i: i + n] for i in range(len(text) - n)]\n",
    "\n",
    "    model_keys = model.keys()\n",
    "    entropy = 0 \n",
    "    for i_letter in n_gram:\n",
    "        if i_letter[0:n-1] in model_keys: \n",
    "            i_letter_model = model[i_letter[0:n-1]]\n",
    "            if i_letter[n-1] in i_letter_model.keys():\n",
    "                second_letter_prob = i_letter_model[i_letter[n-1]]\n",
    "                entropy += -np.log2(second_letter_prob)\n",
    "            else:\n",
    "                entropy += 0\n",
    "        else:\n",
    "            entropy += 0\n",
    "    entropy = entropy/len(n_gram)\n",
    "    perplexity_score = 2**(entropy)\n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eZuvfwbI5aGj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6890020370483398\n",
      "0.39804935455322266\n",
      "1.256019115447998\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "vocabulary = preprocess()\n",
    "print(time.time() - start_time)\n",
    "start_time = time.time()\n",
    "n = 2\n",
    "test_dict = lm(n, vocabulary, data_files['en_df'], False)\n",
    "print(time.time() - start_time)\n",
    "start_time = time.time()\n",
    "eval(n,test_dict, data_files['en_df'])\n",
    "print(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "caAxLE9s_fvn"
   },
   "outputs": [],
   "source": [
    "def match(n, add_one, data_files):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "    result_dict = {}\n",
    "    vocabulary = preprocess()\n",
    "    for i_language_model in languages_list:\n",
    "        \n",
    "        i_model = lm(n, vocabulary, data_files[i_language_model], add_one)\n",
    "        result_dict[i_language_model] = {}\n",
    "\n",
    "        for i_language_test in languages_list:\n",
    "            i_language_model_i_score = eval(n, i_model, data_files[i_language_test])\n",
    "            result_dict[i_language_model][i_language_test] = i_language_model_i_score\n",
    "    perlexity_df = pd.DataFrame(result_dict)\n",
    "    return perlexity_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nk32naXyAMdl"
   },
   "outputs": [],
   "source": [
    " \n",
    "def run_match(data_files):\n",
    "    full_model_dict = {}\n",
    "    # for n in range(2,3):\n",
    "\n",
    "    for n in range(1,5):\n",
    "        add_one = True\n",
    "        perlexity_df = match(n, add_one, data_files)\n",
    "        print(f'n = {n}, add_one = {add_one}')\n",
    "        display(perlexity_df)\n",
    "\n",
    "        add_one = False\n",
    "        perlexity_df = match(n, add_one, data_files)\n",
    "        print(f'n = {n}, add_one = {add_one}')\n",
    "        display(perlexity_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = run_match(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be excepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! ls nlp-course\\lm-languages-data-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = f'nlp-course\\lm-languages-data-new'\n",
    "test_csv_files =  glob.glob(test_folder + '\\\\*.csv')\n",
    "test_files =  {}\n",
    "for i_file in test_csv_files:\n",
    "    file_name_with_ending = os.path.basename(i_file)\n",
    "    file_name = os.path.splitext(file_name_with_ending)[0]\n",
    "    test_files[file_name + '_df'] = f'' + i_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qD6IRIQLrlZF"
   },
   "outputs": [],
   "source": [
    "def match_test(n, data_file_path, add_one):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "    #data_file_path = r\"C:\\MSC\\NLP2\\nlp-course\\lm-languages-data-new\\test.csv\"\n",
    "    senstences_list = pd.read_csv(data_file_path)['tweet_text'].to_list()\n",
    "\n",
    "    lines = [] \n",
    "    result_dict = {}\n",
    "\n",
    "    for i_language_model in languages_list:\n",
    "        # i_model = model_dict[n][add_one][i_language_model]\n",
    "        result_dict[i_language_model] = {}\n",
    "        i_model = lm(n, vocabulary, data_files[i_language_model], add_one)\n",
    "\n",
    "        for i_test_senstence_idx in range(senstences_list.__len__()):\n",
    "            i_test_senstence = senstences_list[i_test_senstence_idx]\n",
    "            i_sentence_model_i_score = eval(n, i_model, i_test_senstence)\n",
    "            result_dict[i_language_model][i_test_senstence_idx] = i_sentence_model_i_score\n",
    "    # print('summary for '+ i_language_model +' model perlexity score for each language:\\n')\n",
    "    perlexity_df = pd.DataFrame(result_dict)\n",
    "    print(perlexity_df)\n",
    "    perlexity_array = perlexity_df.to_numpy()\n",
    "    language_match_index = np.argmin(perlexity_array, axis=1)\n",
    "    language_match_list = reorder_list(languages_list, language_match_index)\n",
    "    perlexity_df['predict'] = language_match_index\n",
    "    perlexity_df['predict_language'] = language_match_list\n",
    "    print(perlexity_df)\n",
    "\n",
    "    return perlexity_df\n",
    "\n",
    "\n",
    "def classify(n, data_file_path, add_one):\n",
    "    match_dict  = match_test(n, data_file_path, add_one)\n",
    "    return match_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
      "0     15.409292  21.675997  22.549406  21.072941  23.192828  21.254515   \n",
      "1     24.244849  22.182511  25.856457  27.546027  15.147335  33.462189   \n",
      "2     17.863943  18.479226  19.762191  16.692756  19.148692  20.950789   \n",
      "3     22.042157  24.368556  22.542958  25.302117  25.746159  17.382716   \n",
      "4     19.857199  20.026403  21.486305  16.308162  19.489727  21.173554   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "7994  19.252089  17.205413  16.864522  21.784607  19.486260  20.629273   \n",
      "7995  26.752657  30.025929  29.577340  13.889901  30.787122  26.901810   \n",
      "7996  32.763242  29.295657  37.072243  33.311929  19.761095  47.049839   \n",
      "7997  15.647607  12.289813  13.221803  18.759703  14.728798  16.033215   \n",
      "7998  20.324670  29.155601  27.911392  24.074354  30.462845  26.633268   \n",
      "\n",
      "          pt_df      tl_df  \n",
      "0     22.419618  19.905563  \n",
      "1     21.028282  24.877526  \n",
      "2     18.150711  15.941840  \n",
      "3     24.785795  25.679442  \n",
      "4     21.705226  14.759738  \n",
      "...         ...        ...  \n",
      "7994  18.919699  22.295273  \n",
      "7995  30.110963  19.657111  \n",
      "7996  26.416423  29.023664  \n",
      "7997  11.422088  19.791445  \n",
      "7998  30.638090  25.955126  \n",
      "\n",
      "[7999 rows x 8 columns]\n",
      "          en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
      "0     15.409292  21.675997  22.549406  21.072941  23.192828  21.254515   \n",
      "1     24.244849  22.182511  25.856457  27.546027  15.147335  33.462189   \n",
      "2     17.863943  18.479226  19.762191  16.692756  19.148692  20.950789   \n",
      "3     22.042157  24.368556  22.542958  25.302117  25.746159  17.382716   \n",
      "4     19.857199  20.026403  21.486305  16.308162  19.489727  21.173554   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "7994  19.252089  17.205413  16.864522  21.784607  19.486260  20.629273   \n",
      "7995  26.752657  30.025929  29.577340  13.889901  30.787122  26.901810   \n",
      "7996  32.763242  29.295657  37.072243  33.311929  19.761095  47.049839   \n",
      "7997  15.647607  12.289813  13.221803  18.759703  14.728798  16.033215   \n",
      "7998  20.324670  29.155601  27.911392  24.074354  30.462845  26.633268   \n",
      "\n",
      "          pt_df      tl_df  predict predict_language  \n",
      "0     22.419618  19.905563        0            en_df  \n",
      "1     21.028282  24.877526        4            it_df  \n",
      "2     18.150711  15.941840        7            tl_df  \n",
      "3     24.785795  25.679442        5            nl_df  \n",
      "4     21.705226  14.759738        7            tl_df  \n",
      "...         ...        ...      ...              ...  \n",
      "7994  18.919699  22.295273        2            fr_df  \n",
      "7995  30.110963  19.657111        3            in_df  \n",
      "7996  26.416423  29.023664        4            it_df  \n",
      "7997  11.422088  19.791445        6            pt_df  \n",
      "7998  30.638090  25.955126        0            en_df  \n",
      "\n",
      "[7999 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "test_path = test_folder + '\\\\test.csv'\n",
    "clasification_result = classify(2, test_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.read_csv(test_path).get('label').to_list()\n",
    "y_true = list(map(lambda x: languages_list.index(x+'_df'),y_true))\n",
    "y_pred = clasification_result['predict'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VOBO3YQls66r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F-score we acheive is 0.785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calc_f1(y_true,y_pred ):\n",
    "    return np.round(f1_score(y_true, y_pred,average=\"micro\"),3)\n",
    "f_score_result = calc_f1(y_true,y_pred)\n",
    "print('The F-score we acheive is ' + str(f_score_result)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "# **Good luck!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
