{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import time\n",
    "import glob\n",
    "import os \n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xC-87z2GWMq",
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nlp-course' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kfirbar/nlp-course.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls nlp-course/lm-languages-data-new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'en_df': 'en.csv',\n",
    "              'es_df': 'es.csv',\n",
    "              'fr_df': 'fr.csv',\n",
    "              'in_df': 'in.csv',\n",
    "              'it_df': 'it.csv',\n",
    "              'nl_df': 'nl.csv',\n",
    "              'pt_df': 'pt.csv',\n",
    "              'tl_df': 'tl.csv'}\n",
    "\n",
    "    \n",
    "directory = 'nlp-course/lm-languages-data-new/'    \n",
    "for (key, value) in data_files.items():\n",
    "    data_files[key] = directory + value\n",
    "    \n",
    "languages_list = list(data_files.keys())\n",
    "\n",
    "start_token = '↠'\n",
    "end_token = '↞'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xCfzsITW8Yaj"
   },
   "outputs": [],
   "source": [
    "def preprocess(data_files):\n",
    "    \"\"\"\n",
    "    data frame is table from 2 columns:\n",
    "        1. tweet id\n",
    "        2. tweet text\n",
    "    \"\"\"  \n",
    "    tokens = []\n",
    "    for path in data_files.values():\n",
    "        df = pd.read_csv(path)\n",
    "        if tokens.__len__() == 0 :\n",
    "            columns_list = df.columns.to_list()\n",
    "        for text in df[columns_list[-1]].values:\n",
    "            tokens.extend(list(text))\n",
    "    return list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = preprocess(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "def tweets_to_text(data_file_path, n):\n",
    "    \"\"\"\n",
    "    data frame is table from 2 columns:\n",
    "        1. tweet id\n",
    "        2. tweet text\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(r''+ data_file_path)\n",
    "    debug = False\n",
    "    if debug == True:\n",
    "        df = df[0:100]\n",
    "    columns_list = df.columns.to_list()\n",
    "    tweets_list = df[columns_list[-1]].apply(lambda x: start_token + x + end_token).values\n",
    "    text = ''.join(tweets_list)\n",
    "    \n",
    "    text = start_token * (n-1) + text + end_token * (n-1)\n",
    "\n",
    "    return text\n",
    "def reorder_list(List, index_list):\n",
    "    return [List[i] for i in index_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "kMC_u8eQbVvZ"
   },
   "outputs": [],
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
    "    # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
    "    # data_file_path - the data_file from which we record probabilities for our model\n",
    "    # add_one - True/False (use add_one smoothing or not)\n",
    "  \n",
    "    lm_dict = {}\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    text = tweets_to_text(data_file_path, n)\n",
    "\n",
    "    # Extract n - 1 length substrings\n",
    "    n_1_gram = [text[i: i + n-1] for i in range(len(text) - (n-1))]\n",
    "    counter_obj_n_1_gram = dict(Counter(n_1_gram))\n",
    "\n",
    "    # Extract n length substrings\n",
    "    n_gram = [text[i: i + n] for i in range(len(text) - n)]\n",
    "    counter_obj_n_gram = dict(Counter(n_gram))\n",
    "\n",
    "    for key in counter_obj_n_1_gram.keys():\n",
    "        inner_dict = {}\n",
    "        if add_one:\n",
    "            gen = (key_1 for key_1 in counter_obj_n_gram.keys() if key_1[0:n-1] == key)\n",
    "            for key_1 in gen:\n",
    "                val = (int(counter_obj_n_gram[key_1]) + 1) / (int(counter_obj_n_1_gram[key]) + V)\n",
    "                inner_dict[key_1[-1]] = val\n",
    "\n",
    "            gen = (token for token in vocabulary if not(token in inner_dict))\n",
    "            for key_1 in gen:\n",
    "                val = 1 /  (int(counter_obj_n_1_gram[key]) + V)\n",
    "                inner_dict[key_1[-1]] = val\n",
    "\n",
    "        else:\n",
    "            gen = (key_1 for key_1 in counter_obj_n_gram.keys() if key_1[0:n-1] == key)\n",
    "            sum_vals = 0\n",
    "            for key_1 in gen:\n",
    "                val = int(counter_obj_n_gram[key_1]) / int(counter_obj_n_1_gram[key])\n",
    "                inner_dict[key_1[-1]] = val\n",
    "                sum_vals += val\n",
    "\n",
    "        lm_dict[key] = inner_dict.copy()\n",
    "\n",
    "    return lm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lm = lm(2, vocabulary, data_files['en_df'], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "F0kkMn328-lJ"
   },
   "outputs": [],
   "source": [
    "def eval(n, model, data_file):\n",
    "    # n - the n-gram that you used to build your model (must be the same number)\n",
    "    # model - the dictionary (model) to use for calculating perplexity\n",
    "    # data_file - the tweets file that you wish to claculate a perplexity score for\n",
    "\n",
    "    # read file\n",
    "    if os.path.exists(data_file):\n",
    "        text = tweets_to_text(data_file, n)\n",
    "    else:\n",
    "        text = data_file\n",
    "    # Extract n length substrings\n",
    "    n_gram = [text[i: i + n] for i in range(len(text) - n)]\n",
    "\n",
    "    model_keys = model.keys()\n",
    "    entropy = 0 \n",
    "    for i_letter in n_gram:\n",
    "        if i_letter[0:n-1] in model_keys: \n",
    "            i_letter_model = model[i_letter[0:n-1]]\n",
    "            if i_letter[n-1] in i_letter_model.keys():\n",
    "                second_letter_prob = i_letter_model[i_letter[n-1]]\n",
    "                entropy += -np.log2(second_letter_prob)\n",
    "            else:\n",
    "                entropy += 0\n",
    "        else:\n",
    "            entropy += 0\n",
    "    entropy = entropy/len(n_gram)\n",
    "    perplexity_score = 2**(entropy)\n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.73587741775965"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(2, test_lm, data_files['en_df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "caAxLE9s_fvn"
   },
   "outputs": [],
   "source": [
    "def match(n, add_one, data_files):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "    result_dict = {}\n",
    "    #vocabulary = preprocess(data_files)\n",
    "    for i_language_model in languages_list:\n",
    "        \n",
    "        i_model = lm(n, vocabulary, data_files[i_language_model], add_one)\n",
    "        result_dict[i_language_model] = {}\n",
    "\n",
    "        for i_language_test in languages_list:\n",
    "            i_language_model_i_score = eval(n, i_model, data_files[i_language_test])\n",
    "            result_dict[i_language_model][i_language_test] = i_language_model_i_score\n",
    "    perlexity_df = pd.DataFrame(result_dict)\n",
    "    return perlexity_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "nk32naXyAMdl"
   },
   "outputs": [],
   "source": [
    " \n",
    "def run_match(data_files):\n",
    "    full_model_dict = {}\n",
    "    # for n in range(2,3):\n",
    "\n",
    "    for n in range(1,5):\n",
    "        add_one = True\n",
    "        perlexity_df = match(n, add_one, data_files)\n",
    "        print(f'n = {n}, add_one = {add_one}')\n",
    "        display(perlexity_df)\n",
    "\n",
    "        add_one = False\n",
    "        perlexity_df = match(n, add_one, data_files)\n",
    "        print(f'n = {n}, add_one = {add_one}')\n",
    "        display(perlexity_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "n = 1, add_one = True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_df</th>\n",
       "      <th>es_df</th>\n",
       "      <th>fr_df</th>\n",
       "      <th>in_df</th>\n",
       "      <th>it_df</th>\n",
       "      <th>nl_df</th>\n",
       "      <th>pt_df</th>\n",
       "      <th>tl_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_df</th>\n",
       "      <td>38.630607</td>\n",
       "      <td>42.062439</td>\n",
       "      <td>41.630300</td>\n",
       "      <td>42.540889</td>\n",
       "      <td>41.498486</td>\n",
       "      <td>40.790634</td>\n",
       "      <td>42.542629</td>\n",
       "      <td>42.187752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_df</th>\n",
       "      <td>40.802257</td>\n",
       "      <td>36.248897</td>\n",
       "      <td>39.694951</td>\n",
       "      <td>43.286563</td>\n",
       "      <td>38.918135</td>\n",
       "      <td>40.690781</td>\n",
       "      <td>37.529098</td>\n",
       "      <td>42.726271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_df</th>\n",
       "      <td>42.736129</td>\n",
       "      <td>40.896944</td>\n",
       "      <td>37.583780</td>\n",
       "      <td>46.985917</td>\n",
       "      <td>40.526005</td>\n",
       "      <td>41.867611</td>\n",
       "      <td>40.683119</td>\n",
       "      <td>47.511581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_df</th>\n",
       "      <td>41.629066</td>\n",
       "      <td>43.776304</td>\n",
       "      <td>44.589019</td>\n",
       "      <td>37.553346</td>\n",
       "      <td>43.570273</td>\n",
       "      <td>41.669084</td>\n",
       "      <td>43.104367</td>\n",
       "      <td>39.224852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it_df</th>\n",
       "      <td>40.671132</td>\n",
       "      <td>40.132135</td>\n",
       "      <td>39.945922</td>\n",
       "      <td>43.329293</td>\n",
       "      <td>37.725009</td>\n",
       "      <td>40.997348</td>\n",
       "      <td>40.562942</td>\n",
       "      <td>42.745835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl_df</th>\n",
       "      <td>39.718389</td>\n",
       "      <td>41.492501</td>\n",
       "      <td>40.919205</td>\n",
       "      <td>41.848385</td>\n",
       "      <td>41.084247</td>\n",
       "      <td>37.610331</td>\n",
       "      <td>41.625927</td>\n",
       "      <td>42.777840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt_df</th>\n",
       "      <td>43.364948</td>\n",
       "      <td>39.777221</td>\n",
       "      <td>41.232575</td>\n",
       "      <td>45.617500</td>\n",
       "      <td>41.306291</td>\n",
       "      <td>42.919685</td>\n",
       "      <td>37.213175</td>\n",
       "      <td>45.019894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_df</th>\n",
       "      <td>44.850369</td>\n",
       "      <td>47.334181</td>\n",
       "      <td>49.266270</td>\n",
       "      <td>42.752191</td>\n",
       "      <td>46.498825</td>\n",
       "      <td>46.506471</td>\n",
       "      <td>47.208401</td>\n",
       "      <td>40.832146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
       "en_df  38.630607  42.062439  41.630300  42.540889  41.498486  40.790634   \n",
       "es_df  40.802257  36.248897  39.694951  43.286563  38.918135  40.690781   \n",
       "fr_df  42.736129  40.896944  37.583780  46.985917  40.526005  41.867611   \n",
       "in_df  41.629066  43.776304  44.589019  37.553346  43.570273  41.669084   \n",
       "it_df  40.671132  40.132135  39.945922  43.329293  37.725009  40.997348   \n",
       "nl_df  39.718389  41.492501  40.919205  41.848385  41.084247  37.610331   \n",
       "pt_df  43.364948  39.777221  41.232575  45.617500  41.306291  42.919685   \n",
       "tl_df  44.850369  47.334181  49.266270  42.752191  46.498825  46.506471   \n",
       "\n",
       "           pt_df      tl_df  \n",
       "en_df  42.542629  42.187752  \n",
       "es_df  37.529098  42.726271  \n",
       "fr_df  40.683119  47.511581  \n",
       "in_df  43.104367  39.224852  \n",
       "it_df  40.562942  42.745835  \n",
       "nl_df  41.625927  42.777840  \n",
       "pt_df  37.213175  45.019894  \n",
       "tl_df  47.208401  40.832146  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, add_one = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_df</th>\n",
       "      <th>es_df</th>\n",
       "      <th>fr_df</th>\n",
       "      <th>in_df</th>\n",
       "      <th>it_df</th>\n",
       "      <th>nl_df</th>\n",
       "      <th>pt_df</th>\n",
       "      <th>tl_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_df</th>\n",
       "      <td>38.578430</td>\n",
       "      <td>41.412538</td>\n",
       "      <td>40.878488</td>\n",
       "      <td>41.934916</td>\n",
       "      <td>40.635953</td>\n",
       "      <td>39.941618</td>\n",
       "      <td>41.730430</td>\n",
       "      <td>41.324155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_df</th>\n",
       "      <td>38.309949</td>\n",
       "      <td>36.193704</td>\n",
       "      <td>39.057459</td>\n",
       "      <td>37.859935</td>\n",
       "      <td>38.449889</td>\n",
       "      <td>39.533533</td>\n",
       "      <td>37.082731</td>\n",
       "      <td>41.968109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_df</th>\n",
       "      <td>39.394444</td>\n",
       "      <td>38.854103</td>\n",
       "      <td>37.534046</td>\n",
       "      <td>44.793916</td>\n",
       "      <td>39.015645</td>\n",
       "      <td>41.087767</td>\n",
       "      <td>39.254733</td>\n",
       "      <td>45.022928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_df</th>\n",
       "      <td>40.831635</td>\n",
       "      <td>42.844921</td>\n",
       "      <td>43.722744</td>\n",
       "      <td>37.497330</td>\n",
       "      <td>42.561710</td>\n",
       "      <td>40.746303</td>\n",
       "      <td>42.144046</td>\n",
       "      <td>38.403995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it_df</th>\n",
       "      <td>38.464632</td>\n",
       "      <td>38.563296</td>\n",
       "      <td>39.276591</td>\n",
       "      <td>41.336244</td>\n",
       "      <td>37.661463</td>\n",
       "      <td>39.965662</td>\n",
       "      <td>38.813462</td>\n",
       "      <td>40.745813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl_df</th>\n",
       "      <td>39.289492</td>\n",
       "      <td>40.806654</td>\n",
       "      <td>40.372793</td>\n",
       "      <td>41.295066</td>\n",
       "      <td>40.484631</td>\n",
       "      <td>37.553020</td>\n",
       "      <td>40.871053</td>\n",
       "      <td>42.211687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt_df</th>\n",
       "      <td>38.403357</td>\n",
       "      <td>36.071470</td>\n",
       "      <td>38.173631</td>\n",
       "      <td>39.073688</td>\n",
       "      <td>37.460498</td>\n",
       "      <td>39.864344</td>\n",
       "      <td>37.141313</td>\n",
       "      <td>43.158697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_df</th>\n",
       "      <td>44.130653</td>\n",
       "      <td>46.466069</td>\n",
       "      <td>48.493727</td>\n",
       "      <td>42.143952</td>\n",
       "      <td>45.622656</td>\n",
       "      <td>45.647622</td>\n",
       "      <td>46.216320</td>\n",
       "      <td>40.761592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
       "en_df  38.578430  41.412538  40.878488  41.934916  40.635953  39.941618   \n",
       "es_df  38.309949  36.193704  39.057459  37.859935  38.449889  39.533533   \n",
       "fr_df  39.394444  38.854103  37.534046  44.793916  39.015645  41.087767   \n",
       "in_df  40.831635  42.844921  43.722744  37.497330  42.561710  40.746303   \n",
       "it_df  38.464632  38.563296  39.276591  41.336244  37.661463  39.965662   \n",
       "nl_df  39.289492  40.806654  40.372793  41.295066  40.484631  37.553020   \n",
       "pt_df  38.403357  36.071470  38.173631  39.073688  37.460498  39.864344   \n",
       "tl_df  44.130653  46.466069  48.493727  42.143952  45.622656  45.647622   \n",
       "\n",
       "           pt_df      tl_df  \n",
       "en_df  41.730430  41.324155  \n",
       "es_df  37.082731  41.968109  \n",
       "fr_df  39.254733  45.022928  \n",
       "in_df  42.144046  38.403995  \n",
       "it_df  38.813462  40.745813  \n",
       "nl_df  40.871053  42.211687  \n",
       "pt_df  37.141313  43.158697  \n",
       "tl_df  46.216320  40.761592  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time\n",
    "model_dict = run_match(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be excepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls nlp-course\\lm-languages-data-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = f'nlp-course\\lm-languages-data-new'\n",
    "test_csv_files =  glob.glob(test_folder + '\\\\*.csv')\n",
    "test_files =  {}\n",
    "for i_file in test_csv_files:\n",
    "    file_name_with_ending = os.path.basename(i_file)\n",
    "    file_name = os.path.splitext(file_name_with_ending)[0]\n",
    "    test_files[file_name + '_df'] = f'' + i_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD6IRIQLrlZF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def match_test(n, data_file_path, add_one):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "    #data_file_path = r\"C:\\MSC\\NLP2\\nlp-course\\lm-languages-data-new\\test.csv\"\n",
    "    senstences_list = pd.read_csv(data_file_path)['tweet_text'].to_list()\n",
    "\n",
    "    lines = [] \n",
    "    result_dict = {}\n",
    "\n",
    "    for i_language_model in languages_list:\n",
    "        # i_model = model_dict[n][add_one][i_language_model]\n",
    "        result_dict[i_language_model] = {}\n",
    "        i_model = lm(n, vocabulary, data_files[i_language_model], add_one)\n",
    "\n",
    "        for i_test_senstence_idx in range(senstences_list.__len__()):\n",
    "            i_test_senstence = senstences_list[i_test_senstence_idx]\n",
    "            i_sentence_model_i_score = eval(n, i_model, i_test_senstence)\n",
    "            result_dict[i_language_model][i_test_senstence_idx] = i_sentence_model_i_score\n",
    "    # print('summary for '+ i_language_model +' model perlexity score for each language:\\n')\n",
    "    perlexity_df = pd.DataFrame(result_dict)\n",
    "    print(perlexity_df)\n",
    "    perlexity_array = perlexity_df.to_numpy()\n",
    "    language_match_index = np.argmin(perlexity_array, axis=1)\n",
    "    language_match_list = reorder_list(languages_list, language_match_index)\n",
    "    perlexity_df['predict'] = language_match_index\n",
    "    perlexity_df['predict_language'] = language_match_list\n",
    "    print(perlexity_df)\n",
    "\n",
    "    #TODO\n",
    "    return perlexity_df\n",
    "\n",
    "\n",
    "def classify(n, data_file_path, add_one):\n",
    "    # TODO\n",
    "    match_dict  = match_test(n, data_file_path, add_one)\n",
    "    return match_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "test_path = test_folder + '\\\\test.csv'\n",
    "clasification_result = classify(2, test_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.read_csv(test_path).get('label').to_list()\n",
    "y_true = list(map(lambda x: languages_list.index(x+'_df'),y_true))\n",
    "y_pred = clasification_result['predict'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOBO3YQls66r"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_f1(y_true,y_pred ):\n",
    "    return np.round(f1_score(y_true, y_pred,average=\"micro\"),3)\n",
    "f_score_result = calc_f1(y_true,y_pred)\n",
    "print('The F-score we acheive is ' + str(f_score_result)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "# **Good luck!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
