{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import time\n",
    "import glob\n",
    "import os \n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xC-87z2GWMq",
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nlp-course' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kfirbar/nlp-course.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls nlp-course/lm-languages-data-new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'en_df': 'en.csv',\n",
    "              'es_df': 'es.csv',\n",
    "              'fr_df': 'fr.csv',\n",
    "              'in_df': 'in.csv',\n",
    "              'it_df': 'it.csv',\n",
    "              'nl_df': 'nl.csv',\n",
    "              'pt_df': 'pt.csv',\n",
    "              'tl_df': 'tl.csv'}\n",
    "\n",
    "    \n",
    "directory = 'nlp-course/lm-languages-data-new/'    \n",
    "for (key, value) in data_files.items():\n",
    "    data_files[key] = directory + value\n",
    "    \n",
    "languages_list = list(data_files.keys())\n",
    "\n",
    "start_token = '↠'\n",
    "end_token = '↞'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xCfzsITW8Yaj"
   },
   "outputs": [],
   "source": [
    "def preprocess(data_files):\n",
    "    \"\"\"\n",
    "    data frame is table from 2 columns:\n",
    "        1. tweet id\n",
    "        2. tweet text\n",
    "    \"\"\"  \n",
    "    tokens = []\n",
    "    for path in data_files.values():\n",
    "        df = pd.read_csv(path)\n",
    "        if tokens.__len__() == 0 :\n",
    "            columns_list = df.columns.to_list()\n",
    "        for text in df[columns_list[-1]].values:\n",
    "            tokens.extend(list(text))\n",
    "    return list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = preprocess(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "def tweets_to_text(data_file_path, n):\n",
    "    \"\"\"\n",
    "    data frame is table from 2 columns:\n",
    "        1. tweet id\n",
    "        2. tweet text\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(r''+ data_file_path)\n",
    "    debug = False\n",
    "    if debug == True:\n",
    "        df = df[0:100]\n",
    "    columns_list = df.columns.to_list()\n",
    "    tweets_list = df[columns_list[-1]].apply(lambda x: start_token + x + end_token).values\n",
    "    text = ''.join(tweets_list)\n",
    "    \n",
    "    text = start_token * (n-1) + text + end_token * (n-1)\n",
    "\n",
    "    return text\n",
    "def reorder_list(List, index_list):\n",
    "    return [List[i] for i in index_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "kMC_u8eQbVvZ"
   },
   "outputs": [],
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
    "    # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
    "    # data_file_path - the data_file from which we record probabilities for our model\n",
    "    # add_one - True/False (use add_one smoothing or not)\n",
    "  \n",
    "    lm_dict = {}\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    text = tweets_to_text(data_file_path, n)\n",
    "\n",
    "    # Extract n - 1 length substrings\n",
    "    n_1_gram = [text[i: i + n-1] for i in range(len(text) - (n-1))]\n",
    "    counter_obj_n_1_gram = dict(Counter(n_1_gram))\n",
    "\n",
    "    # Extract n length substrings\n",
    "    n_gram = [text[i: i + n] for i in range(len(text) - n)]\n",
    "    counter_obj_n_gram = dict(Counter(n_gram))\n",
    "\n",
    "    for key in counter_obj_n_1_gram.keys():\n",
    "        inner_dict = {}\n",
    "        if add_one:\n",
    "            gen = (key_1 for key_1 in counter_obj_n_gram.keys() if key_1[0:n-1] == key)\n",
    "            for key_1 in gen:\n",
    "                val = (int(counter_obj_n_gram[key_1]) + 1) / (int(counter_obj_n_1_gram[key]) + V)\n",
    "                inner_dict[key_1[-1]] = val\n",
    "\n",
    "            gen = (token for token in vocabulary if not(token in inner_dict))\n",
    "            for key_1 in gen:\n",
    "                val = 1 /  (int(counter_obj_n_1_gram[key]) + V)\n",
    "                inner_dict[key_1[-1]] = val\n",
    "\n",
    "        else:\n",
    "            gen = (key_1 for key_1 in counter_obj_n_gram.keys() if key_1[0:n-1] == key)\n",
    "            sum_vals = 0\n",
    "            for key_1 in gen:\n",
    "                val = int(counter_obj_n_gram[key_1]) / int(counter_obj_n_1_gram[key])\n",
    "                inner_dict[key_1[-1]] = val\n",
    "                sum_vals += val\n",
    "\n",
    "        lm_dict[key] = inner_dict.copy()\n",
    "\n",
    "    return lm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lm = lm(2, vocabulary, data_files['en_df'], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "F0kkMn328-lJ"
   },
   "outputs": [],
   "source": [
    "def eval(n, model, data_file):\n",
    "    # n - the n-gram that you used to build your model (must be the same number)\n",
    "    # model - the dictionary (model) to use for calculating perplexity\n",
    "    # data_file - the tweets file that you wish to claculate a perplexity score for\n",
    "\n",
    "    # read file\n",
    "    if os.path.exists(data_file):\n",
    "        text = tweets_to_text(data_file, n)\n",
    "    else:\n",
    "        text = data_file\n",
    "    # Extract n length substrings\n",
    "    n_gram = [text[i: i + n] for i in range(len(text) - n)]\n",
    "\n",
    "    model_keys = model.keys()\n",
    "    entropy = 0 \n",
    "    for i_letter in n_gram:\n",
    "        if i_letter[0:n-1] in model_keys: \n",
    "            i_letter_model = model[i_letter[0:n-1]]\n",
    "            if i_letter[n-1] in i_letter_model.keys():\n",
    "                second_letter_prob = i_letter_model[i_letter[n-1]]\n",
    "                entropy += -np.log2(second_letter_prob)\n",
    "            else:\n",
    "                entropy += 0\n",
    "        else:\n",
    "            entropy += 0\n",
    "    entropy = entropy/len(n_gram)\n",
    "    perplexity_score = 2**(entropy)\n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.73587741775965"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(2, test_lm, data_files['en_df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "caAxLE9s_fvn"
   },
   "outputs": [],
   "source": [
    "def match(n, add_one, data_files):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "    result_dict = {}\n",
    "    #vocabulary = preprocess(data_files)\n",
    "    for i_language_model in languages_list:\n",
    "        \n",
    "        i_model = lm(n, vocabulary, data_files[i_language_model], add_one)\n",
    "        result_dict[i_language_model] = {}\n",
    "\n",
    "        for i_language_test in languages_list:\n",
    "            i_language_model_i_score = eval(n, i_model, data_files[i_language_test])\n",
    "            result_dict[i_language_model][i_language_test] = i_language_model_i_score\n",
    "    perlexity_df = pd.DataFrame(result_dict)\n",
    "    return perlexity_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "nk32naXyAMdl"
   },
   "outputs": [],
   "source": [
    " \n",
    "def run_match(data_files):\n",
    "    full_model_dict = {}\n",
    "    # for n in range(2,3):\n",
    "\n",
    "    for n in range(1,5):\n",
    "        add_one = True\n",
    "        #perlexity_df = match(n, add_one, data_files)\n",
    "        #print(f'n = {n}, add_one = {add_one}')\n",
    "        #display(perlexity_df)\n",
    "\n",
    "        add_one = False\n",
    "        perlexity_df = match(n, add_one, data_files)\n",
    "        print(f'n = {n}, add_one = {add_one}')\n",
    "        display(perlexity_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "n = 1, add_one = True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_df</th>\n",
       "      <th>es_df</th>\n",
       "      <th>fr_df</th>\n",
       "      <th>in_df</th>\n",
       "      <th>it_df</th>\n",
       "      <th>nl_df</th>\n",
       "      <th>pt_df</th>\n",
       "      <th>tl_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_df</th>\n",
       "      <td>38.630607</td>\n",
       "      <td>42.062439</td>\n",
       "      <td>41.630300</td>\n",
       "      <td>42.540889</td>\n",
       "      <td>41.498486</td>\n",
       "      <td>40.790634</td>\n",
       "      <td>42.542629</td>\n",
       "      <td>42.187752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_df</th>\n",
       "      <td>40.802257</td>\n",
       "      <td>36.248897</td>\n",
       "      <td>39.694951</td>\n",
       "      <td>43.286563</td>\n",
       "      <td>38.918135</td>\n",
       "      <td>40.690781</td>\n",
       "      <td>37.529098</td>\n",
       "      <td>42.726271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_df</th>\n",
       "      <td>42.736129</td>\n",
       "      <td>40.896944</td>\n",
       "      <td>37.583780</td>\n",
       "      <td>46.985917</td>\n",
       "      <td>40.526005</td>\n",
       "      <td>41.867611</td>\n",
       "      <td>40.683119</td>\n",
       "      <td>47.511581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_df</th>\n",
       "      <td>41.629066</td>\n",
       "      <td>43.776304</td>\n",
       "      <td>44.589019</td>\n",
       "      <td>37.553346</td>\n",
       "      <td>43.570273</td>\n",
       "      <td>41.669084</td>\n",
       "      <td>43.104367</td>\n",
       "      <td>39.224852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it_df</th>\n",
       "      <td>40.671132</td>\n",
       "      <td>40.132135</td>\n",
       "      <td>39.945922</td>\n",
       "      <td>43.329293</td>\n",
       "      <td>37.725009</td>\n",
       "      <td>40.997348</td>\n",
       "      <td>40.562942</td>\n",
       "      <td>42.745835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl_df</th>\n",
       "      <td>39.718389</td>\n",
       "      <td>41.492501</td>\n",
       "      <td>40.919205</td>\n",
       "      <td>41.848385</td>\n",
       "      <td>41.084247</td>\n",
       "      <td>37.610331</td>\n",
       "      <td>41.625927</td>\n",
       "      <td>42.777840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt_df</th>\n",
       "      <td>43.364948</td>\n",
       "      <td>39.777221</td>\n",
       "      <td>41.232575</td>\n",
       "      <td>45.617500</td>\n",
       "      <td>41.306291</td>\n",
       "      <td>42.919685</td>\n",
       "      <td>37.213175</td>\n",
       "      <td>45.019894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_df</th>\n",
       "      <td>44.850369</td>\n",
       "      <td>47.334181</td>\n",
       "      <td>49.266270</td>\n",
       "      <td>42.752191</td>\n",
       "      <td>46.498825</td>\n",
       "      <td>46.506471</td>\n",
       "      <td>47.208401</td>\n",
       "      <td>40.832146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
       "en_df  38.630607  42.062439  41.630300  42.540889  41.498486  40.790634   \n",
       "es_df  40.802257  36.248897  39.694951  43.286563  38.918135  40.690781   \n",
       "fr_df  42.736129  40.896944  37.583780  46.985917  40.526005  41.867611   \n",
       "in_df  41.629066  43.776304  44.589019  37.553346  43.570273  41.669084   \n",
       "it_df  40.671132  40.132135  39.945922  43.329293  37.725009  40.997348   \n",
       "nl_df  39.718389  41.492501  40.919205  41.848385  41.084247  37.610331   \n",
       "pt_df  43.364948  39.777221  41.232575  45.617500  41.306291  42.919685   \n",
       "tl_df  44.850369  47.334181  49.266270  42.752191  46.498825  46.506471   \n",
       "\n",
       "           pt_df      tl_df  \n",
       "en_df  42.542629  42.187752  \n",
       "es_df  37.529098  42.726271  \n",
       "fr_df  40.683119  47.511581  \n",
       "in_df  43.104367  39.224852  \n",
       "it_df  40.562942  42.745835  \n",
       "nl_df  41.625927  42.777840  \n",
       "pt_df  37.213175  45.019894  \n",
       "tl_df  47.208401  40.832146  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, add_one = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_df</th>\n",
       "      <th>es_df</th>\n",
       "      <th>fr_df</th>\n",
       "      <th>in_df</th>\n",
       "      <th>it_df</th>\n",
       "      <th>nl_df</th>\n",
       "      <th>pt_df</th>\n",
       "      <th>tl_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_df</th>\n",
       "      <td>38.578430</td>\n",
       "      <td>41.412538</td>\n",
       "      <td>40.878488</td>\n",
       "      <td>41.934916</td>\n",
       "      <td>40.635953</td>\n",
       "      <td>39.941618</td>\n",
       "      <td>41.730430</td>\n",
       "      <td>41.324155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_df</th>\n",
       "      <td>38.309949</td>\n",
       "      <td>36.193704</td>\n",
       "      <td>39.057459</td>\n",
       "      <td>37.859935</td>\n",
       "      <td>38.449889</td>\n",
       "      <td>39.533533</td>\n",
       "      <td>37.082731</td>\n",
       "      <td>41.968109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_df</th>\n",
       "      <td>39.394444</td>\n",
       "      <td>38.854103</td>\n",
       "      <td>37.534046</td>\n",
       "      <td>44.793916</td>\n",
       "      <td>39.015645</td>\n",
       "      <td>41.087767</td>\n",
       "      <td>39.254733</td>\n",
       "      <td>45.022928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_df</th>\n",
       "      <td>40.831635</td>\n",
       "      <td>42.844921</td>\n",
       "      <td>43.722744</td>\n",
       "      <td>37.497330</td>\n",
       "      <td>42.561710</td>\n",
       "      <td>40.746303</td>\n",
       "      <td>42.144046</td>\n",
       "      <td>38.403995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it_df</th>\n",
       "      <td>38.464632</td>\n",
       "      <td>38.563296</td>\n",
       "      <td>39.276591</td>\n",
       "      <td>41.336244</td>\n",
       "      <td>37.661463</td>\n",
       "      <td>39.965662</td>\n",
       "      <td>38.813462</td>\n",
       "      <td>40.745813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl_df</th>\n",
       "      <td>39.289492</td>\n",
       "      <td>40.806654</td>\n",
       "      <td>40.372793</td>\n",
       "      <td>41.295066</td>\n",
       "      <td>40.484631</td>\n",
       "      <td>37.553020</td>\n",
       "      <td>40.871053</td>\n",
       "      <td>42.211687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt_df</th>\n",
       "      <td>38.403357</td>\n",
       "      <td>36.071470</td>\n",
       "      <td>38.173631</td>\n",
       "      <td>39.073688</td>\n",
       "      <td>37.460498</td>\n",
       "      <td>39.864344</td>\n",
       "      <td>37.141313</td>\n",
       "      <td>43.158697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_df</th>\n",
       "      <td>44.130653</td>\n",
       "      <td>46.466069</td>\n",
       "      <td>48.493727</td>\n",
       "      <td>42.143952</td>\n",
       "      <td>45.622656</td>\n",
       "      <td>45.647622</td>\n",
       "      <td>46.216320</td>\n",
       "      <td>40.761592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
       "en_df  38.578430  41.412538  40.878488  41.934916  40.635953  39.941618   \n",
       "es_df  38.309949  36.193704  39.057459  37.859935  38.449889  39.533533   \n",
       "fr_df  39.394444  38.854103  37.534046  44.793916  39.015645  41.087767   \n",
       "in_df  40.831635  42.844921  43.722744  37.497330  42.561710  40.746303   \n",
       "it_df  38.464632  38.563296  39.276591  41.336244  37.661463  39.965662   \n",
       "nl_df  39.289492  40.806654  40.372793  41.295066  40.484631  37.553020   \n",
       "pt_df  38.403357  36.071470  38.173631  39.073688  37.460498  39.864344   \n",
       "tl_df  44.130653  46.466069  48.493727  42.143952  45.622656  45.647622   \n",
       "\n",
       "           pt_df      tl_df  \n",
       "en_df  41.730430  41.324155  \n",
       "es_df  37.082731  41.968109  \n",
       "fr_df  39.254733  45.022928  \n",
       "in_df  42.144046  38.403995  \n",
       "it_df  38.813462  40.745813  \n",
       "nl_df  40.871053  42.211687  \n",
       "pt_df  37.141313  43.158697  \n",
       "tl_df  46.216320  40.761592  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2, add_one = True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_df</th>\n",
       "      <th>es_df</th>\n",
       "      <th>fr_df</th>\n",
       "      <th>in_df</th>\n",
       "      <th>it_df</th>\n",
       "      <th>nl_df</th>\n",
       "      <th>pt_df</th>\n",
       "      <th>tl_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_df</th>\n",
       "      <td>20.670583</td>\n",
       "      <td>31.351690</td>\n",
       "      <td>27.721354</td>\n",
       "      <td>29.013545</td>\n",
       "      <td>30.874923</td>\n",
       "      <td>26.598121</td>\n",
       "      <td>32.620053</td>\n",
       "      <td>26.862259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_df</th>\n",
       "      <td>26.369529</td>\n",
       "      <td>18.529989</td>\n",
       "      <td>24.782305</td>\n",
       "      <td>27.848169</td>\n",
       "      <td>24.005736</td>\n",
       "      <td>29.968519</td>\n",
       "      <td>23.026733</td>\n",
       "      <td>27.901591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_df</th>\n",
       "      <td>27.712458</td>\n",
       "      <td>27.749668</td>\n",
       "      <td>19.456992</td>\n",
       "      <td>33.228314</td>\n",
       "      <td>28.511183</td>\n",
       "      <td>29.980050</td>\n",
       "      <td>28.635958</td>\n",
       "      <td>32.797383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_df</th>\n",
       "      <td>28.830641</td>\n",
       "      <td>33.243302</td>\n",
       "      <td>31.897969</td>\n",
       "      <td>20.776453</td>\n",
       "      <td>32.618576</td>\n",
       "      <td>29.501577</td>\n",
       "      <td>34.897671</td>\n",
       "      <td>25.054035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it_df</th>\n",
       "      <td>26.478284</td>\n",
       "      <td>23.805529</td>\n",
       "      <td>25.642331</td>\n",
       "      <td>27.977729</td>\n",
       "      <td>18.993601</td>\n",
       "      <td>29.857890</td>\n",
       "      <td>25.169702</td>\n",
       "      <td>26.746126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl_df</th>\n",
       "      <td>27.043887</td>\n",
       "      <td>32.356294</td>\n",
       "      <td>28.995091</td>\n",
       "      <td>29.184426</td>\n",
       "      <td>32.558026</td>\n",
       "      <td>20.223090</td>\n",
       "      <td>34.083291</td>\n",
       "      <td>30.396873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt_df</th>\n",
       "      <td>28.725020</td>\n",
       "      <td>23.463504</td>\n",
       "      <td>26.558819</td>\n",
       "      <td>30.519313</td>\n",
       "      <td>25.288476</td>\n",
       "      <td>31.807602</td>\n",
       "      <td>19.416773</td>\n",
       "      <td>30.787865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_df</th>\n",
       "      <td>28.335177</td>\n",
       "      <td>33.171902</td>\n",
       "      <td>33.756436</td>\n",
       "      <td>26.022783</td>\n",
       "      <td>32.131217</td>\n",
       "      <td>31.028266</td>\n",
       "      <td>34.691937</td>\n",
       "      <td>20.868752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
       "en_df  20.670583  31.351690  27.721354  29.013545  30.874923  26.598121   \n",
       "es_df  26.369529  18.529989  24.782305  27.848169  24.005736  29.968519   \n",
       "fr_df  27.712458  27.749668  19.456992  33.228314  28.511183  29.980050   \n",
       "in_df  28.830641  33.243302  31.897969  20.776453  32.618576  29.501577   \n",
       "it_df  26.478284  23.805529  25.642331  27.977729  18.993601  29.857890   \n",
       "nl_df  27.043887  32.356294  28.995091  29.184426  32.558026  20.223090   \n",
       "pt_df  28.725020  23.463504  26.558819  30.519313  25.288476  31.807602   \n",
       "tl_df  28.335177  33.171902  33.756436  26.022783  32.131217  31.028266   \n",
       "\n",
       "           pt_df      tl_df  \n",
       "en_df  32.620053  26.862259  \n",
       "es_df  23.026733  27.901591  \n",
       "fr_df  28.635958  32.797383  \n",
       "in_df  34.897671  25.054035  \n",
       "it_df  25.169702  26.746126  \n",
       "nl_df  34.083291  30.396873  \n",
       "pt_df  19.416773  30.787865  \n",
       "tl_df  34.691937  20.868752  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 2, add_one = False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_df</th>\n",
       "      <th>es_df</th>\n",
       "      <th>fr_df</th>\n",
       "      <th>in_df</th>\n",
       "      <th>it_df</th>\n",
       "      <th>nl_df</th>\n",
       "      <th>pt_df</th>\n",
       "      <th>tl_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en_df</th>\n",
       "      <td>17.735877</td>\n",
       "      <td>25.321927</td>\n",
       "      <td>22.932261</td>\n",
       "      <td>23.663476</td>\n",
       "      <td>25.324383</td>\n",
       "      <td>22.151118</td>\n",
       "      <td>25.400508</td>\n",
       "      <td>21.979446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es_df</th>\n",
       "      <td>19.219495</td>\n",
       "      <td>15.771597</td>\n",
       "      <td>18.132484</td>\n",
       "      <td>20.509963</td>\n",
       "      <td>18.296791</td>\n",
       "      <td>22.124516</td>\n",
       "      <td>18.236226</td>\n",
       "      <td>20.344066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr_df</th>\n",
       "      <td>19.398425</td>\n",
       "      <td>20.526563</td>\n",
       "      <td>16.666102</td>\n",
       "      <td>21.442419</td>\n",
       "      <td>21.174813</td>\n",
       "      <td>22.599347</td>\n",
       "      <td>21.275432</td>\n",
       "      <td>21.245718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_df</th>\n",
       "      <td>24.030153</td>\n",
       "      <td>26.744017</td>\n",
       "      <td>26.029441</td>\n",
       "      <td>17.538527</td>\n",
       "      <td>26.324807</td>\n",
       "      <td>24.517132</td>\n",
       "      <td>27.329411</td>\n",
       "      <td>20.684112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it_df</th>\n",
       "      <td>21.329857</td>\n",
       "      <td>18.992255</td>\n",
       "      <td>21.113126</td>\n",
       "      <td>21.730244</td>\n",
       "      <td>16.131021</td>\n",
       "      <td>24.063558</td>\n",
       "      <td>19.337399</td>\n",
       "      <td>20.834061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl_df</th>\n",
       "      <td>22.713120</td>\n",
       "      <td>26.404655</td>\n",
       "      <td>24.136791</td>\n",
       "      <td>23.949707</td>\n",
       "      <td>26.769307</td>\n",
       "      <td>17.405674</td>\n",
       "      <td>27.060856</td>\n",
       "      <td>24.806232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt_df</th>\n",
       "      <td>20.096012</td>\n",
       "      <td>18.246442</td>\n",
       "      <td>19.786148</td>\n",
       "      <td>21.171551</td>\n",
       "      <td>18.547024</td>\n",
       "      <td>22.924194</td>\n",
       "      <td>15.982621</td>\n",
       "      <td>20.442436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl_df</th>\n",
       "      <td>23.208788</td>\n",
       "      <td>26.228006</td>\n",
       "      <td>26.858127</td>\n",
       "      <td>21.000203</td>\n",
       "      <td>25.503613</td>\n",
       "      <td>25.022696</td>\n",
       "      <td>26.428730</td>\n",
       "      <td>17.321968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           en_df      es_df      fr_df      in_df      it_df      nl_df  \\\n",
       "en_df  17.735877  25.321927  22.932261  23.663476  25.324383  22.151118   \n",
       "es_df  19.219495  15.771597  18.132484  20.509963  18.296791  22.124516   \n",
       "fr_df  19.398425  20.526563  16.666102  21.442419  21.174813  22.599347   \n",
       "in_df  24.030153  26.744017  26.029441  17.538527  26.324807  24.517132   \n",
       "it_df  21.329857  18.992255  21.113126  21.730244  16.131021  24.063558   \n",
       "nl_df  22.713120  26.404655  24.136791  23.949707  26.769307  17.405674   \n",
       "pt_df  20.096012  18.246442  19.786148  21.171551  18.547024  22.924194   \n",
       "tl_df  23.208788  26.228006  26.858127  21.000203  25.503613  25.022696   \n",
       "\n",
       "           pt_df      tl_df  \n",
       "en_df  25.400508  21.979446  \n",
       "es_df  18.236226  20.344066  \n",
       "fr_df  21.275432  21.245718  \n",
       "in_df  27.329411  20.684112  \n",
       "it_df  19.337399  20.834061  \n",
       "nl_df  27.060856  24.806232  \n",
       "pt_df  15.982621  20.442436  \n",
       "tl_df  26.428730  17.321968  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time\n",
    "model_dict = run_match(data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be excepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls nlp-course\\lm-languages-data-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = f'nlp-course\\lm-languages-data-new'\n",
    "test_csv_files =  glob.glob(test_folder + '\\\\*.csv')\n",
    "test_files =  {}\n",
    "for i_file in test_csv_files:\n",
    "    file_name_with_ending = os.path.basename(i_file)\n",
    "    file_name = os.path.splitext(file_name_with_ending)[0]\n",
    "    test_files[file_name + '_df'] = f'' + i_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD6IRIQLrlZF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def match_test(n, data_file_path, add_one):\n",
    "    # n - the n-gram to use for creating n-gram models\n",
    "    # add_one - use add_one smoothing or not\n",
    "    #data_file_path = r\"C:\\MSC\\NLP2\\nlp-course\\lm-languages-data-new\\test.csv\"\n",
    "    senstences_list = pd.read_csv(data_file_path)['tweet_text'].to_list()\n",
    "\n",
    "    lines = [] \n",
    "    result_dict = {}\n",
    "\n",
    "    for i_language_model in languages_list:\n",
    "        # i_model = model_dict[n][add_one][i_language_model]\n",
    "        result_dict[i_language_model] = {}\n",
    "        i_model = lm(n, vocabulary, data_files[i_language_model], add_one)\n",
    "\n",
    "        for i_test_senstence_idx in range(senstences_list.__len__()):\n",
    "            i_test_senstence = senstences_list[i_test_senstence_idx]\n",
    "            i_sentence_model_i_score = eval(n, i_model, i_test_senstence)\n",
    "            result_dict[i_language_model][i_test_senstence_idx] = i_sentence_model_i_score\n",
    "    # print('summary for '+ i_language_model +' model perlexity score for each language:\\n')\n",
    "    perlexity_df = pd.DataFrame(result_dict)\n",
    "    print(perlexity_df)\n",
    "    perlexity_array = perlexity_df.to_numpy()\n",
    "    language_match_index = np.argmin(perlexity_array, axis=1)\n",
    "    language_match_list = reorder_list(languages_list, language_match_index)\n",
    "    perlexity_df['predict'] = language_match_index\n",
    "    perlexity_df['predict_language'] = language_match_list\n",
    "    print(perlexity_df)\n",
    "\n",
    "    #TODO\n",
    "    return perlexity_df\n",
    "\n",
    "\n",
    "def classify(n, data_file_path, add_one):\n",
    "    # TODO\n",
    "    match_dict  = match_test(n, data_file_path, add_one)\n",
    "    return match_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "test_path = test_folder + '\\\\test.csv'\n",
    "clasification_result = classify(2, test_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.read_csv(test_path).get('label').to_list()\n",
    "y_true = list(map(lambda x: languages_list.index(x+'_df'),y_true))\n",
    "y_pred = clasification_result['predict'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOBO3YQls66r"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_f1(y_true,y_pred ):\n",
    "    return np.round(f1_score(y_true, y_pred,average=\"micro\"),3)\n",
    "f_score_result = calc_f1(y_true,y_pred)\n",
    "print('The F-score we acheive is ' + str(f_score_result)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "# **Good luck!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
