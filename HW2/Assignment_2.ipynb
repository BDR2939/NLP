{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zB2BpE6DhW"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "This assignment is about training and evaluating a POS tagger with some real data. The dataset is available through the Universal Dependencies (https://universaldependencies.org/) (UD) project. To get to know the project, please visit https://universaldependencies.org/introduction.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA7ZRDZj3V0_",
        "outputId": "58a27b03-15be-4dcf-a694-244ec801e87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllutils\n",
            "  Downloading conllutils-1.1.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from conllutils) (1.21.6)\n",
            "Building wheels for collected packages: conllutils\n",
            "  Building wheel for conllutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for conllutils: filename=conllutils-1.1.4-py3-none-any.whl size=17697 sha256=8e11a4a7c109125bf3206fecca92681e780eb73442e509e0bf29c0a31a1f42a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/9c/af/495f50326290abb66f82ac92273619cdad168cc1b79af379db\n",
            "Successfully built conllutils\n",
            "Installing collected packages: conllutils\n",
            "Successfully installed conllutils-1.1.4\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.4.2\n",
            "Collecting conll-df\n",
            "  Downloading conll_df-0.0.4.tar.gz (3.5 kB)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from conll-df) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->conll-df) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->conll-df) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->conll-df) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19.2->conll-df) (1.15.0)\n",
            "Building wheels for collected packages: conll-df\n",
            "  Building wheel for conll-df (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for conll-df\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for conll-df\n",
            "Failed to build conll-df\n",
            "Installing collected packages: conll-df\n",
            "    Running setup.py install for conll-df ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: conll-df was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
            "Successfully installed conll-df-0.0.4\n"
          ]
        }
      ],
      "source": [
        "! pip install conllutils\n",
        "! pip install conllu\n",
        "! pip install conll-df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "iRm7zcfq56HF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import operator\n",
        "import nltk\n",
        "\n",
        "import conllutils\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "from collections import defaultdict\n",
        "from conllutils import pipe\n",
        "from conll_df import conll_df\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-Xvqip6Teu"
      },
      "source": [
        "**Part 1** (getting the data)\n",
        "\n",
        "You can download the dataset files directly from the UD website, but it will let you only download all the languages in one compressed file. In this assignment you will be working with th GUM dataset, which you can download directly from:\n",
        "https://github.com/UniversalDependencies/UD_English-GUM.\n",
        "Please download it to your colab machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsZsyTVC6Sw0",
        "outputId": "f4d25aed-6b18-4222-a879-02244893ff1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UD_English-GUM'...\n",
            "remote: Enumerating objects: 3888, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (184/184), done.\u001b[K\n",
            "remote: Total 3888 (delta 149), reused 11 (delta 6), pack-reused 3698\u001b[K\n",
            "Receiving objects: 100% (3888/3888), 36.19 MiB | 17.66 MiB/s, done.\n",
            "Resolving deltas: 100% (3499/3499), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_English-GUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZGOtoteWHz"
      },
      "source": [
        "We will use the (train/dev/test) files:\n",
        "\n",
        "UD_English-GUM/en_gum-ud-train.conllu\n",
        "\n",
        "UD_English-GUM/en_gum-ud-dev.conllu\n",
        "\n",
        "UD_English-GUM/en_gum-ud-test.conllu\n",
        "\n",
        "They are all formatted in the conllu format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n",
        "\n",
        "You should write a code that reads the three datasets into memory. \n",
        "You may choose the data structure by yourself. \n",
        "As you can see\n",
        "1. every word is represented by a line\n",
        "2. columns representing specific features. \n",
        "   * We are only interested in the first and fourth columns \n",
        "   * corresponding to the word and its POS tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gh4qtav3V1E"
      },
      "source": [
        "### Set Path's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v7A0-DjWg2JW"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "FOLDER = 'UD_English-GUM'\n",
        "\n",
        "\n",
        "\n",
        "user = 'va'\n",
        "if user == 'Or':\n",
        "    ud_dev =   r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-dev.conllu\"\n",
        "    ud_train = r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-train.conllu\"\n",
        "    ud_test =  r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-test.conllu\"\n",
        "elif user == 'Roni':\n",
        "    ud_dev =   '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-dev.conllu'\n",
        "    ud_train = '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-train.conllu'\n",
        "    ud_test =  '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-test.conllu'\n",
        "else:\n",
        "    ud_dev =   f'{FOLDER}/en_gum-ud-dev.conllu'\n",
        "    ud_train = f'{FOLDER}/en_gum-ud-train.conllu'\n",
        "    ud_test =  f'{FOLDER}/en_gum-ud-test.conllu'\n",
        "\n",
        "train_csv = FOLDER + '/en_gum-ud-train.csv'\n",
        "test_csv = FOLDER + '/en_gum-ud-test.csv'\n",
        "dev_csv = FOLDER + '/en_gum-ud-dev.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conllu(file_path):\n",
        "    # w = []\n",
        "    # p = []\n",
        "    # index = []\n",
        "    # s = []\n",
        "    # i = []\n",
        "    row = []\n",
        "    data_file = open(file_path, \"r\", encoding=\"utf-8\")\n",
        "    for sentence_ind, tokenlist in enumerate(parse_incr(data_file), 1):\n",
        "        for token in tokenlist:\n",
        "          row.append([int(sentence_ind),f'{token[\"id\"]}', str(token['form']),str(token['xpos']) ])\n",
        "            # w.append(str(token['form']))\n",
        "            # p.append(str(token['xpos']))\n",
        "            # s.append(sentence_ind)\n",
        "            # i.append(int(token[\"id\"]))\n",
        "            # index.append((sentence_ind, f'{token[\"id\"]}'))\n",
        "    \n",
        "    df = pd.DataFrame(row, columns = ['s', 'i', 'w','p'])\n",
        "    # df.index = index\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "kUIt9SGQ6dqu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KyXIP4H3V1F"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "rF6GfFbV3V1G"
      },
      "outputs": [],
      "source": [
        "# train_df = conll_df(ud_train, file_index=False)\n",
        "train_df = read_conllu(ud_train)\n",
        "# train_df = train_df.iloc[:, [0, 3]]\n",
        "\n",
        "# dev_df = conll_df(ud_dev, file_index=False)\n",
        "dev_df = read_conllu(ud_dev)\n",
        "# dev_df = dev_df.iloc[:, [0, 3]]\n",
        "\n",
        "# test_df = conll_df(ud_test, file_index=False)\n",
        "test_df = read_conllu(ud_test)\n",
        "# test_df = test_df.iloc[:, [0, 3]]\n",
        "\n",
        "# train_df.to_csv(train_csv)\n",
        "# test_df.to_csv(test_csv)\n",
        "# dev_df.to_csv(dev_csv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "HO38Dvgh8aYy",
        "outputId": "1c521d8e-f201-45aa-ddcc-972c8dd82561"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           s  i             w   p\n",
              "0          1  1     Aesthetic  JJ\n",
              "1          1  2  Appreciation  NN\n",
              "2          1  3           and  CC\n",
              "3          1  4       Spanish  JJ\n",
              "4          1  5           Art  NN\n",
              "...      ... ..           ...  ..\n",
              "104688  5660  3            or  CC\n",
              "104689  5660  4      strainer  NN\n",
              "104690  5660  5            to  TO\n",
              "104691  5660  6          hold  VB\n",
              "104692  5660  7        filter  NN\n",
              "\n",
              "[104693 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7dc17e0-b12f-4ecd-90de-07dacc224312\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s</th>\n",
              "      <th>i</th>\n",
              "      <th>w</th>\n",
              "      <th>p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Aesthetic</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Appreciation</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Spanish</td>\n",
              "      <td>JJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Art</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104688</th>\n",
              "      <td>5660</td>\n",
              "      <td>3</td>\n",
              "      <td>or</td>\n",
              "      <td>CC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104689</th>\n",
              "      <td>5660</td>\n",
              "      <td>4</td>\n",
              "      <td>strainer</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104690</th>\n",
              "      <td>5660</td>\n",
              "      <td>5</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104691</th>\n",
              "      <td>5660</td>\n",
              "      <td>6</td>\n",
              "      <td>hold</td>\n",
              "      <td>VB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104692</th>\n",
              "      <td>5660</td>\n",
              "      <td>7</td>\n",
              "      <td>filter</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>104693 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7dc17e0-b12f-4ecd-90de-07dacc224312')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7dc17e0-b12f-4ecd-90de-07dacc224312 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7dc17e0-b12f-4ecd-90de-07dacc224312');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "K0wq55qc5jRv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_df = pd.read_csv(train_csv)\n",
        "# dev_df = pd.read_csv(dev_csv)\n",
        "# test_df = pd.read_csv(test_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Tfqqluah3V1H"
      },
      "outputs": [],
      "source": [
        "def extract_ommision_matrix_B(train_df, unique_pos, words_indexing_dict):\n",
        "\n",
        "    B = np.zeros([len(unique_pos), len(words_indexing_dict)])\n",
        "    B_row_index = 0\n",
        "    for i_pos in unique_pos:\n",
        "        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
        "        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
        "        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
        "        for i_word in i_pos_words:\n",
        "            updated_percent_per_word_per_pos = i_pos_percent[np.where(i_pos_words == i_word)[0][0]]\n",
        "            B_column_word_index = words_indexing_dict[i_word]\n",
        "            B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
        "        B_row_index += 1\n",
        "\n",
        "    return B\n",
        "\n",
        "def generate_transition_matrix_A_and_pi(train_df, unique_pos, pos_indexing_dict):\n",
        "    A = np.zeros((len(unique_pos), len(unique_pos)))\n",
        "    pi = np.zeros(([len(unique_pos), 1]))\n",
        "\n",
        "    sentence_ind = 1\n",
        "    pi[pos_indexing_dict[train_df.iloc[0, :]['p']]] += 1\n",
        "\n",
        "    for i in range(1, train_df.shape[0]):\n",
        "        curr_sentence_ind = train_df.index[i][0]\n",
        "        if curr_sentence_ind != sentence_ind:\n",
        "            pi[pos_indexing_dict[train_df.iloc[i, :]['p']]] += 1\n",
        "            sentence_ind += 1\n",
        "        else:\n",
        "            A[pos_indexing_dict[train_df.iloc[i-1, :]['p']], pos_indexing_dict[train_df.iloc[i, :]['p']]] += 1\n",
        "    \n",
        "    A = A/A.sum(axis=1, keepdims=True)\n",
        "    pi = pi / sum(pi)\n",
        "\n",
        "    return A, pi\n",
        "\n",
        "def generate_transition_matrix_A(train_df, unique_pos, unique_words):\n",
        "    unique_words_list  = unique_words.tolist()\n",
        "    unique_pos_list  = unique_pos.tolist()\n",
        "    A = np.zeros([len(unique_pos), len(unique_pos)])\n",
        "    rol_train_df = train_df.copy()\n",
        "    rol_train_df = rol_train_df.iloc[np.arange(-1, len(rol_train_df) - 1)].reset_index(drop=True)\n",
        "    rol_train_df = rol_train_df[1:]\n",
        "    A_row_index = 0\n",
        "    for i_pos in unique_pos:\n",
        "        index_2_slice = (train_df['p'] == i_pos)\n",
        "        index_2_slice = index_2_slice.iloc[np.arange(-1, len(index_2_slice) - 1)].reset_index(drop=True)\n",
        "        index_2_slice.iloc[0] = False\n",
        "        i_pos_train_df = train_df.loc[index_2_slice]\n",
        "\n",
        "        i_pos_pos, i_pos_pos_count = np.unique(i_pos_train_df.loc[:, 'p'].values, return_counts=True)\n",
        "        i_pos_percent = i_pos_pos_count / np.sum(i_pos_pos_count)\n",
        "        i_pos_pos_list = i_pos_pos.tolist()\n",
        "        for i_pos_next in i_pos_pos_list:\n",
        "            if i_pos_next in i_pos_pos_list:\n",
        "                pos_index = i_pos_pos_list.index(i_pos_next)\n",
        "                updated_percent_per_pos_per_pos = i_pos_percent[pos_index]\n",
        "                A_column_pos_index = unique_pos_list.index(i_pos_next)\n",
        "                A[A_row_index, A_column_pos_index] = updated_percent_per_pos_per_pos\n",
        "            else:\n",
        "                continue\n",
        "        A_row_index += 1\n",
        "    return A\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_ommision_matrix_B_2(train_df, unique_pos, unique_words):\n",
        "    unique_words_list = unique_words.tolist()\n",
        "    unique_pos_list = unique_pos.tolist()\n",
        "\n",
        "    B = np.zeros([len(unique_pos), len(unique_words)])\n",
        "    B_row_index = 0\n",
        "    for i_pos in unique_pos:\n",
        "        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
        "        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
        "        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
        "        i_pos_words_list = i_pos_words.tolist()\n",
        "        for i_word in i_pos_words_list:\n",
        "            if i_word in i_pos_words_list:\n",
        "                word_index = i_pos_words_list.index(i_word)\n",
        "                updated_percent_per_word_per_pos = i_pos_percent[word_index]\n",
        "                B_column_word_index = unique_words_list.index(i_word)\n",
        "                B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
        "            else:\n",
        "                continue\n",
        "        B_row_index += 1\n",
        "\n",
        "    return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeK8JGmQ3V1J"
      },
      "source": [
        "### Create matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_rVOEoAu3V1K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "231943fc-13b5-4dbe-f0ec-4da9126cd799"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-adfa3883ffdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Generate transition matrix and initial marix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_initial_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_transition_matrix_A_and_pi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_indexing_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-bdd1b3be0cf3>\u001b[0m in \u001b[0;36mgenerate_transition_matrix_A_and_pi\u001b[0;34m(train_df, unique_pos, pos_indexing_dict)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcurr_sentence_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_sentence_ind\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msentence_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_indexing_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0msentence_ind\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# we may have a nested tuples indexer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3377\u001b[0m         \u001b[0;31m# irow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3379\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;31m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36miget\u001b[0;34m(self, col)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2011\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \"\"\"\n\u001b[0;32m-> 2013\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ### Create matrices\n",
        "# pos_values = list(np.unique(train_df.loc[:, 'p'].values, return_counts=True))\n",
        "# unique_words = np.unique(train_df.loc[:, 'w'].values)\n",
        "# unique_pos = pos_values[0]\n",
        "\n",
        "# words_indexing_dict = {unique_words[i] : i for i in range(len(unique_words))}\n",
        "# pos_indexing_dict = {unique_pos[i] : i for i in range(len(unique_pos))}\n",
        "\n",
        "# # Generate omissiom matrix\n",
        "# B = extract_ommision_matrix_B(train_df, unique_pos, words_indexing_dict)\n",
        "\n",
        "# # Generate transition matrix and initial marix\n",
        "# A, pi_initial_matrix = generate_transition_matrix_A_and_pi(train_df, unique_pos, pos_indexing_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhAV0YAw3V1L"
      },
      "source": [
        "### Create sentences from df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lw5H1AHI3V1L"
      },
      "outputs": [],
      "source": [
        "# def sentences_from_df(df):\n",
        "#     sentences = []\n",
        "#     sentence_ind = 1\n",
        "#     sentence = []\n",
        "\n",
        "#     for i in range(df.shape[0]):\n",
        "#         curr_sentence_ind = df.index[i][0]\n",
        "#         if curr_sentence_ind != sentence_ind:\n",
        "#             sentences.append(str.join(\" \", sentence))\n",
        "#             sentence_ind += 1\n",
        "#             sentence = []\n",
        "#         sentence.append(df.iloc[i, :]['w'])\n",
        "\n",
        "#     return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVW-Dmp33V1M"
      },
      "outputs": [],
      "source": [
        "# sentences = sentences_from_df(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Z9BMNM7EP3"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a class **simple_tagger**\n",
        "1. with methods *train* and *evaluate*. \n",
        "2. The method *train* receives the data as a list of sentences\n",
        "3. use it for training the tagger.\n",
        "4. In this case, it should learn a simple dictionary that maps words to tags\n",
        "    * defined as the most frequent tag for every word (in case there is more than one most frequent tag, you may select one of them randomly).\n",
        "    * The dictionary should be stored as a class member for evaluation.\n",
        "\n",
        "The method *evaluate* \n",
        "1. receives the data as a list of sentences\n",
        "2. use it to evaluate the tagger performance. \n",
        "3. Specifically, you should calculate the word and sentence level accuracy.\n",
        "4. The evaluation process is simply going word by word\n",
        "5. querying the dictionary (created by the train method) for each word’s tag and compare it to the true tag of that word. \n",
        "6. The word-level accuracy is the number of successes divided by the number of words.\n",
        "7. For OOV (out of vocabulary, or unknown) words\n",
        "8. the tagger should assign the most frequent tag in the entire training set (i.e., the mode).\n",
        "9. The function should return the two numbers:\n",
        "    * word level accuracy\n",
        "    * sentence level accuracy.\n",
        "\n",
        "\n",
        "Write a class simple_tagger, with methods train and evaluate. The method train receives the data as a list of sentences, and use it for training the tagger. In this case, it should learn a simple dictionary that maps words to tags, defined as the most frequent tag for every word (in case there is more than one most frequent tag, you may select one of them randomly). The dictionary should be stored as a class member for evaluation.\n",
        "\n",
        "The method evaluate receives the data as a list of sentences, and use it to evaluate the tagger performance. Specifically, you should calculate the word and sentence level accuracy. The evaluation process is simply going word by word, querying the dictionary (created by the train method) for each word’s tag and compare it to the true tag of that word. The word-level accuracy is the number of successes divided by the number of words. For OOV (out of vocabulary, or unknown) words, the tagger should assign the most frequent tag in the entire training set (i.e., the mode). The function should return the two numbers: word level accuracy and sentence level accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "GmNW2n6AsCYO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_list_of_sentences_tag_lists(df):\n",
        "    \n",
        "    df['i'] = pd.to_numeric(df['i'], errors='coerce')\n",
        "    df['s'] = pd.to_numeric(df['s'], errors='coerce')\n",
        "\n",
        "    word_index_array = df['i'].to_numpy()\n",
        "    initial_sentence_idx = np.where(word_index_array == 1)[0]\n",
        "    words_list = df['w'].to_list()\n",
        "    tag_list = df['p'].to_list()\n",
        "\n",
        "    sentence_list  = (initial_sentence_idx.size)*['None']\n",
        "    sentence_tag_list  = (initial_sentence_idx.size)*['None']\n",
        "\n",
        "    for sentence_idx in range(initial_sentence_idx.size):\n",
        "        if not sentence_idx == initial_sentence_idx.size-1:\n",
        "            \n",
        "            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]\n",
        "            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]\n",
        "        else:\n",
        "            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]::]\n",
        "            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]::]\n",
        "        \n",
        "        sentence_list[sentence_idx] = curr_sentence\n",
        "        sentence_tag_list[sentence_idx] = curr_tag\n",
        "\n",
        "    return sentence_list, sentence_tag_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "MtivZLBH7dXq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class simple_tagger:\n",
        "    def __init__(self):\n",
        "        self.tagger = {}\n",
        "    \n",
        "    def train(self, data):\n",
        "        pos = np.unique(data['p'], return_counts=True)\n",
        "        most_frequent_pos = pos[0][np.where(pos[1] == max(pos[1]))][0]\n",
        "\n",
        "        tagger = defaultdict(lambda: most_frequent_pos)\n",
        "\n",
        "        unique_words = np.unique(data.loc[:, 'w'].values)\n",
        "        for word in unique_words:\n",
        "            word_data = data.loc[data['w'] == word]\n",
        "            word_data_count = word_data['p'].value_counts()\n",
        "            predicted_tag = word_data_count.index[0]\n",
        "            # predicted_tag = pos_of_word = np.unique(data.loc[data['w'] == word, 'p'], return_counts=True)\n",
        "            tagger[word] = predicted_tag\n",
        "\n",
        "        self.tagger = tagger\n",
        "    def evaluate(self, data):\n",
        "        # sentences = sentences_from_df(data)\n",
        "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
        "\n",
        "        words_success = 0\n",
        "        sentences_success = 0\n",
        "        idx = pd.IndexSlice\n",
        "        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
        "        # for sentence_num, sentence in enumerate(sentences, 1):\n",
        "            count_successes = 0\n",
        "            # sentence_df = data.loc[idx[sentence_num], 'p'].values\n",
        "            for word_num, (word, actual_tag) in enumerate(zip(sentence, sentence_tags)):\n",
        "            # for word_num, word in enumerate(sentence.split(' ')):\n",
        "                predicted_tag = self.tagger[word]\n",
        "                # actual_tag = sentence_df[word_num]\n",
        "\n",
        "                if predicted_tag == actual_tag:\n",
        "                    words_success += 1\n",
        "                    count_successes += 1\n",
        "\n",
        "            if count_successes == len(sentence)-1:\n",
        "                sentences_success += 1\n",
        "\n",
        "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
        "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
        "\n",
        "        return word_accuracy, sentence_accuracy\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArJn89Hi3V1N"
      },
      "outputs": [],
      "source": [
        "tagger = simple_tagger()\n",
        "tagger.train(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Rg0O7iYU3V1N"
      },
      "outputs": [],
      "source": [
        "simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train = tagger.evaluate(train_df)\n",
        "simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev = tagger.evaluate(dev_df)\n",
        "simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test = tagger.evaluate(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "STiEtEUK3V1N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "cb899ccf-144f-4747-d409-9736f60e0428"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  data-set  word-accuracy[%]  sentence-accuracy[%]\n",
              "0    train           93.0922               29.2756\n",
              "1      dev           84.8731               19.5730\n",
              "2     test           82.5907               18.2327"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9823be5-7e98-4212-91e3-497e2e3cb7f5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data-set</th>\n",
              "      <th>word-accuracy[%]</th>\n",
              "      <th>sentence-accuracy[%]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>93.0922</td>\n",
              "      <td>29.2756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dev</td>\n",
              "      <td>84.8731</td>\n",
              "      <td>19.5730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>82.5907</td>\n",
              "      <td>18.2327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9823be5-7e98-4212-91e3-497e2e3cb7f5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9823be5-7e98-4212-91e3-497e2e3cb7f5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9823be5-7e98-4212-91e3-497e2e3cb7f5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "result_list = [['train' ,simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train],\n",
        "['dev', simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev],\n",
        "['test', simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test]]\n",
        "\n",
        "simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
        "simple_tagger_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK9iZIq8i0X"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Similar to part 2, write the class hmm_tagger, which implements HMM tagging. The method *train* should build the matrices A, B and Pi, from the data as discussed in class. The method *evaluate* should find the best tag sequence for every input sentence using he Viterbi decoding algorithm, and then calculate the word and sentence level accuracy using the gold-standard tags. You should implement the Viterbi algorithm in the next block and call it from your class.\n",
        "\n",
        "Additional guidance:\n",
        "1. The matrix B represents the emissions probabilities. Since B is a matrix, you should build a dictionary that maps every unique word in the corpus to a serial numeric id (starting with 0). This way columns in B represents word ids.\n",
        "2. During the evaluation, you should first convert each word into it’s index and then create the observation array to be given to Viterbi, as a list of ids. OOV words should be assigned with a random tag. To make sure Viterbi works appropriately, you can simply break the sentence into multiple segments every time you see an OOV word, and decode every segment individually using Viterbi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "BYMC--2xsCYR"
      },
      "outputs": [],
      "source": [
        "def dptable(V):\n",
        "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
        "    for state in V[0]:\n",
        "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
        "\n",
        "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
        "             \n",
        "     V = [{}]\n",
        "     for st in states:\n",
        "         V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
        "   \n",
        "     for t in range(1, len(observations)):\n",
        "         V.append({})\n",
        "         for st in states:\n",
        "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
        "            prev_st_selected = states[0]\n",
        "            for prev_st in states[1:]:\n",
        "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
        "                if tr_prob > max_tr_prob:\n",
        "                    max_tr_prob = tr_prob\n",
        "                    prev_st_selected = prev_st\n",
        " \n",
        "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
        "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
        "     \n",
        "     # for line in dptable(V):\n",
        "     #    print(line)\n",
        " \n",
        "     opt = []\n",
        "     max_prob = 0.0\n",
        "     best_st = None\n",
        " \n",
        "     for st, data in V[-1].items():\n",
        "        if data[\"prob\"] > max_prob:\n",
        "            max_prob = data[\"prob\"]\n",
        "            best_st = st\n",
        "     \n",
        "     if best_st == None:\n",
        "         best_st = random.choice(list(V[-1].keys()))\n",
        "     opt.append(best_st)\n",
        "     previous = best_st\n",
        " \n",
        " \n",
        "     for t in range(len(V) - 2, -1, -1):\n",
        "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
        "        previous = V[t + 1][previous][\"prev\"]\n",
        "        \n",
        "     return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "TpH7GuiQ9L6W"
      },
      "outputs": [],
      "source": [
        "   \n",
        "class hmm_tagger:\n",
        "    data = pd.DataFrame()\n",
        "    A = []\n",
        "    B = []\n",
        "    Pi = []\n",
        "    uniqueWordsArr = []\n",
        "    \n",
        "    def __init__(self, data_df):\n",
        "        \n",
        "        unique_pos, pos_counts = list(np.unique(data_df.loc[:, 'p'].values, return_counts=True))\n",
        "        unique_words, word_counts = np.unique(data_df.loc[:, 'w'].values, return_counts=True)\n",
        "        unique_words_list  = unique_words.tolist()\n",
        "        unique_pos_list  = unique_pos.tolist()\n",
        "        \n",
        "        self.data_df = data_df\n",
        "        self.unique_pos = unique_pos\n",
        "        self.unique_words = unique_words\n",
        "        self.unique_words_list = unique_words_list\n",
        "        self.unique_pos_list = unique_pos_list\n",
        "\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "\n",
        "        data_df = self.data_df\n",
        "        unique_pos = self.unique_pos\n",
        "        unique_words = self.unique_words\n",
        "        \n",
        "        \n",
        "        B = extract_ommision_matrix_B_2(self.data_df, unique_pos, unique_words)\n",
        "        A = generate_transition_matrix_A(self.data_df, unique_pos, unique_words)\n",
        "        Pi = np.ones([unique_pos.size,1])*(1/unique_pos.size)\n",
        "\n",
        "        B = pd.DataFrame(B.T, index = unique_words, columns = unique_pos)\n",
        "        A = pd.DataFrame(A.T, index =unique_pos , columns = unique_pos)\n",
        "        Pi = pd.DataFrame(Pi.T, columns = unique_pos)\n",
        "   \n",
        "        self.states = tuple(unique_pos)\n",
        "        self.A = A.to_dict()\n",
        "        self.B = B.to_dict()\n",
        "        self.Pi = Pi.to_dict('records')[0]\n",
        "        \n",
        "        # self.A, self.B, self.Pi = calc_A_B_Pi(data_df)\n",
        "        self.uniqueWordsArr = unique_words.tolist()\n",
        "\n",
        "   \n",
        "    \n",
        "    def evaluate(self, data):\n",
        "        # sentences = sentences_from_df(data)\n",
        "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
        "        sentenceIdx = [] \n",
        "        words_success = 0\n",
        "        sentences_success = 0\n",
        "        idx = pd.IndexSlice\n",
        "        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
        "            # for sentence_num, sentence in enumerate(sentences, 1):\n",
        "            count_successes = 0\n",
        "            \n",
        "            \n",
        "            # validate all word in sentence is in train data set otherwise\n",
        "            # choose randomly word from bank of word\n",
        "            for i_word_idx, word in enumerate(sentence):\n",
        "                if not word in self.unique_words:\n",
        "                    sentence[i_word_idx] = random.choice(self.unique_words_list)\n",
        "                \n",
        "                \n",
        "            predicted_tag_list = viterbi_algorithm(tuple(sentence), self.states, self.Pi, self.A, self.B)  \n",
        "            \n",
        "            for word_num, (predicted_tag, actual_tag) in enumerate(zip(predicted_tag_list, sentence_tags)):\n",
        "                # print('word nun ' + str(word_num))\n",
        "\n",
        "                if predicted_tag == actual_tag:\n",
        "                    words_success += 1\n",
        "                    count_successes += 1\n",
        "\n",
        "            if count_successes == len(sentence)-1:\n",
        "                sentences_success += 1\n",
        "\n",
        "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
        "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
        "\n",
        "        return word_accuracy, sentence_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DR6KJW2F9yqt"
      },
      "outputs": [],
      "source": [
        "\n",
        "hmmTagger = hmm_tagger(train_df)\n",
        "hmmTagger.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train = hmmTagger.evaluate(train_df)\n",
        "hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test = hmmTagger.evaluate(test_df)\n",
        "hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev = hmmTagger.evaluate(dev_df)\n"
      ],
      "metadata": {
        "id": "zvb6qUu6zHru"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "5Q5oYLOFsCYR",
        "outputId": "ebfb2b8e-ba59-4586-f21c-29e5717e633f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  data-set  word-accuracy[%]  sentence-accuracy[%]\n",
              "0    train           97.1440               24.2403\n",
              "1      dev           82.2378               22.6572\n",
              "2     test           81.1582               20.1342"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ab458e2-5ec8-44bf-b46d-b87df06fd44b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data-set</th>\n",
              "      <th>word-accuracy[%]</th>\n",
              "      <th>sentence-accuracy[%]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>97.1440</td>\n",
              "      <td>24.2403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dev</td>\n",
              "      <td>82.2378</td>\n",
              "      <td>22.6572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>81.1582</td>\n",
              "      <td>20.1342</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ab458e2-5ec8-44bf-b46d-b87df06fd44b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ab458e2-5ec8-44bf-b46d-b87df06fd44b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ab458e2-5ec8-44bf-b46d-b87df06fd44b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "\n",
        "\n",
        "result_list = [['train' ,hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train],\n",
        "['dev', hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev],\n",
        "['test', hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test]]\n",
        "\n",
        "simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
        "simple_tagger_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZO0uGL-4S-"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Compare the results obtained from both taggers and a MEMM tagger, implemented by NLTK (a known NLP library), over both, the dev and test datasets. To train the NLTK MEMM tagger you should execute the following lines (it may take some time to train...):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "KYhtboJm_Iyx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "9ca1098c-9f0f-45d4-9087-da7c03b2b141"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-085e63facdfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtnt_pos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTnT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtnt_pos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtnt_pos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/tnt.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BOS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'BOS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;31m# if capitalization is requested,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "from nltk.tag import tnt \n",
        "\n",
        "tnt_pos_tagger = tnt.TnT()\n",
        "tnt_pos_tagger.train(train_df)\n",
        "print(tnt_pos_tagger.evaluate(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIvvzsq_U-o"
      },
      "source": [
        "Print both, word level and sentence level accuracy for all the three taggers in a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V32202cikh7u"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}