{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zB2BpE6DhW"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "This assignment is about training and evaluating a POS tagger with some real data. The dataset is available through the Universal Dependencies (https://universaldependencies.org/) (UD) project. To get to know the project, please visit https://universaldependencies.org/introduction.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA7ZRDZj3V0_",
        "outputId": "58a27b03-15be-4dcf-a694-244ec801e87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: conllutils in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (1.1.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from conllutils) (1.22.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: conllu in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (4.4.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: conll-df in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (0.0.4)\n",
            "Requirement already satisfied: pandas>=0.19.2 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from conll-df) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=0.19.2->conll-df) (1.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=0.19.2->conll-df) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=0.19.2->conll-df) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->conll-df) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install conllutils\n",
        "! pip install conllu\n",
        "! pip install conll-df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iRm7zcfq56HF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-Xvqip6Teu"
      },
      "source": [
        "**Part 1** (getting the data)\n",
        "\n",
        "You can download the dataset files directly from the UD website, but it will let you only download all the languages in one compressed file. In this assignment you will be working with th GUM dataset, which you can download directly from:\n",
        "https://github.com/UniversalDependencies/UD_English-GUM.\n",
        "Please download it to your colab machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsZsyTVC6Sw0",
        "outputId": "f4d25aed-6b18-4222-a879-02244893ff1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'UD_English-GUM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_English-GUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZZGOtoteWHz"
      },
      "source": [
        "We will use the (train/dev/test) files:\n",
        "\n",
        "UD_English-GUM/en_gum-ud-train.conllu\n",
        "\n",
        "UD_English-GUM/en_gum-ud-dev.conllu\n",
        "\n",
        "UD_English-GUM/en_gum-ud-test.conllu\n",
        "\n",
        "They are all formatted in the conllu format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n",
        "\n",
        "You should write a code that reads the three datasets into memory. \n",
        "You may choose the data structure by yourself. \n",
        "As you can see\n",
        "1. every word is represented by a line\n",
        "2. columns representing specific features. \n",
        "   * We are only interested in the first and fourth columns \n",
        "   * corresponding to the word and its POS tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gh4qtav3V1E"
      },
      "source": [
        "### Set Path's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v7A0-DjWg2JW"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "FOLDER = 'UD_English-GUM'\n",
        "\n",
        "user = 'ev'\n",
        "if user == 'Or':\n",
        "    ud_dev =   r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-dev.conllu\"\n",
        "    ud_train = r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-train.conllu\"\n",
        "    ud_test =  r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-test.conllu\"\n",
        "elif user == 'Roni':\n",
        "    ud_dev =   '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-dev.conllu'\n",
        "    ud_train = '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-train.conllu'\n",
        "    ud_test =  '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-test.conllu'\n",
        "else:\n",
        "    ud_dev =   f'{FOLDER}/en_gum-ud-dev.conllu'\n",
        "    ud_train = f'{FOLDER}/en_gum-ud-train.conllu'\n",
        "    ud_test =  f'{FOLDER}/en_gum-ud-test.conllu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kUIt9SGQ6dqu"
      },
      "outputs": [],
      "source": [
        "def read_conllu(file_path):\n",
        "  row = []\n",
        "  data_file = open(file_path, \"r\", encoding=\"utf-8\")\n",
        "  for sentence_ind, tokenlist in enumerate(parse_incr(data_file), 1):\n",
        "      for token in tokenlist:\n",
        "        row.append([int(sentence_ind),f'{token[\"id\"]}', str(token['form']),str(token['xpos']) ])\n",
        "            \n",
        "  df = pd.DataFrame(row, columns = ['s', 'i', 'w','p'])\n",
        "  \n",
        "  return df"
      ]
    },
    "id": "tA7ZRDZj3V0_",
    "outputId": "58a27b03-15be-4dcf-a694-244ec801e87e"
  },
  "outputs": [
    {
      "name": "stdout",
      "output_type": "stream",
      "text": [
        "Defaulting to user installation because normal site-packages is not writeable\n",
        "Requirement already satisfied: conllutils in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (1.1.4)\n",
        "Requirement already satisfied: numpy in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from conllutils) (1.22.0)\n",
        "Defaulting to user installation because normal site-packages is not writeable\n",
        "Requirement already satisfied: conllu in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (4.4.1)\n",
        "Defaulting to user installation because normal site-packages is not writeable\n",
        "Requirement already satisfied: conll-df in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (0.0.4)\n",
        "Requirement already satisfied: pandas>=0.19.2 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from conll-df) (1.4.1)\n",
        "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=0.19.2->conll-df) (2.8.2)\n",
        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=0.19.2->conll-df) (2022.1)\n",
        "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=0.19.2->conll-df) (1.22.0)\n",
        "Requirement already satisfied: six>=1.5 in c:\\users\\cw8822\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->conll-df) (1.16.0)\n"
      ]
    }
  ],
  "source": [
    "! pip install conllutils\n",
    "! pip install conllu\n",
    "! pip install conll-df\n"
  ]
},
{
  "cell_type": "code",
  "execution_count": 2,
  "metadata": {
    "id": "iRm7zcfq56HF"
  },
  "outputs": [],
  "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n"
  ]
},
{
  "cell_type": "markdown",
  "metadata": {
    "id": "UH-Xvqip6Teu"
  },
  "source": [
    "**Part 1** (getting the data)\n",
    "\n",
    "You can download the dataset files directly from the UD website, but it will let you only download all the languages in one compressed file. In this assignment you will be working with th GUM dataset, which you can download directly from:\n",
    "https://github.com/UniversalDependencies/UD_English-GUM.\n",
    "Please download it to your colab machine.\n",
    "\n"
  ]
},
{
  "cell_type": "code",
  "execution_count": 3,
  "metadata": {
    "colab": {
      "base_uri": "https://localhost:8080/"
    },
    "id": "nsZsyTVC6Sw0",
    "outputId": "f4d25aed-6b18-4222-a879-02244893ff1f"
  },
  "outputs": [
    {
      "name": "stderr",
      "output_type": "stream",
      "text": [
        "fatal: destination path 'UD_English-GUM' already exists and is not an empty directory.\n"
      ]
    }
  ],
  "source": [
    "!git clone https://github.com/UniversalDependencies/UD_English-GUM"
  ]
},
{
  "cell_type": "markdown",
  "metadata": {
    "id": "CZZGOtoteWHz"
  },
  "source": [
    "We will use the (train/dev/test) files:\n",
    "\n",
    "UD_English-GUM/en_gum-ud-train.conllu\n",
    "\n",
    "UD_English-GUM/en_gum-ud-dev.conllu\n",
    "\n",
    "UD_English-GUM/en_gum-ud-test.conllu\n",
    "\n",
    "They are all formatted in the conllu format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n",
    "\n",
    "You should write a code that reads the three datasets into memory. \n",
    "You may choose the data structure by yourself. \n",
    "As you can see\n",
    "1. every word is represented by a line\n",
    "2. columns representing specific features. \n",
    "   * We are only interested in the first and fourth columns \n",
    "   * corresponding to the word and its POS tag."
  ]
},
{
  "cell_type": "markdown",
  "metadata": {
    "id": "6Gh4qtav3V1E"
  },
  "source": [
    "### Set Path's"
  ]
},
{
  "cell_type": "code",
  "execution_count": 4,
  "metadata": {
    "id": "v7A0-DjWg2JW"
  },
  "outputs": [],
  "source": [
    "# Your code goes here\n",
    "FOLDER = 'UD_English-GUM'\n",
    "\n",
    "user = 'ev'\n",
    "if user == 'Or':\n",
    "    ud_dev =   r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-dev.conllu\"\n",
    "    ud_train = r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-train.conllu\"\n",
    "    ud_test =  r\"C:\\MSC\\NLP2\\HW2\\UD_English-GUM\\en_gum-ud-test.conllu\"\n",
    "elif user == 'Roni':\n",
    "    ud_dev =   '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-dev.conllu'\n",
    "    ud_train = '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-train.conllu'\n",
    "    ud_test =  '/Users/ronibendom/Master/NLP/HW2/UD_English-GUM/en_gum-ud-test.conllu'\n",
    "else:\n",
    "    ud_dev =   f'{FOLDER}/en_gum-ud-dev.conllu'\n",
    "    ud_train = f'{FOLDER}/en_gum-ud-train.conllu'\n",
    "    ud_test =  f'{FOLDER}/en_gum-ud-test.conllu'\n"
  ]
},
{
  "cell_type": "code",
  "execution_count": 5,
  "metadata": {
    "id": "kUIt9SGQ6dqu"
  },
  "outputs": [],
  "source": [
    "def read_conllu(file_path):\n",
    "  row = []\n",
    "  data_file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "  for sentence_ind, tokenlist in enumerate(parse_incr(data_file), 1):\n",
    "      for token in tokenlist:\n",
    "        row.append([int(sentence_ind),f'{token[\"id\"]}', str(token['form']),str(token['xpos']) ])\n",
    "            \n",
    "  df = pd.DataFrame(row, columns = ['s', 'i', 'w','p'])\n",
    "  \n",
    "  return df"
  ]
},
{
  "cell_type": "markdown",
  "metadata": {
    "id": "6KyXIP4H3V1F"
  },
  "source": [
    "### Get Data"
  ]
},
{
  "cell_type": "code",
  "execution_count": 6,
  "metadata": {
    "id": "rF6GfFbV3V1G"
  },
  "outputs": [],
  "source": [
    "train_df = read_conllu(ud_train)\n",
    "\n",
    "dev_df = read_conllu(ud_dev)\n",
    "\n",
    "test_df = read_conllu(ud_test)"
  ]
},
{
  "cell_type": "code",
  "execution_count": 7,
  "metadata": {
    "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 423
    },
    "id": "HO38Dvgh8aYy",
    "outputId": "1c521d8e-f201-45aa-ddcc-972c8dd82561"
  },
  "outputs": [
    {
      "data": {
        "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>s</th>\n",
          "      <th>i</th>\n",
          "      <th>w</th>\n",
          "      <th>p</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>1</td>\n",
          "      <td>1</td>\n",
          "      <td>Aesthetic</td>\n",
          "      <td>JJ</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>1</td>\n",
          "      <td>2</td>\n",
          "      <td>Appreciation</td>\n",
          "      <td>NN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>1</td>\n",
          "      <td>3</td>\n",
          "      <td>and</td>\n",
          "      <td>CC</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>3</th>\n",
          "      <td>1</td>\n",
          "      <td>4</td>\n",
          "      <td>Spanish</td>\n",
          "      <td>JJ</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4</th>\n",
          "      <td>1</td>\n",
          "      <td>5</td>\n",
          "      <td>Art</td>\n",
          "      <td>NN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>...</th>\n",
          "      <td>...</td>\n",
          "      <td>...</td>\n",
          "      <td>...</td>\n",
          "      <td>...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>104688</th>\n",
          "      <td>5660</td>\n",
          "      <td>3</td>\n",
          "      <td>or</td>\n",
          "      <td>CC</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>104689</th>\n",
          "      <td>5660</td>\n",
          "      <td>4</td>\n",
          "      <td>strainer</td>\n",
          "      <td>NN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>104690</th>\n",
          "      <td>5660</td>\n",
          "      <td>5</td>\n",
          "      <td>to</td>\n",
          "      <td>TO</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>104691</th>\n",
          "      <td>5660</td>\n",
          "      <td>6</td>\n",
          "      <td>hold</td>\n",
          "      <td>VB</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>104692</th>\n",
          "      <td>5660</td>\n",
          "      <td>7</td>\n",
          "      <td>filter</td>\n",
          "      <td>NN</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "<p>104693 rows × 4 columns</p>\n",
          "</div>"
        ],
        "source": [
          "train_df"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 8,
        "metadata": {
          "id": "Tfqqluah3V1H"
        },
        "outputs": [],
        "source": [
          "def extract_ommision_matrix_B(train_df, unique_pos, words_indexing_dict):\n",
          "    \"\"\"\n",
          "    input\n",
          "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
          "     2. unique_pos - all tags base train data\n",
          "     3. words_indexing_dict - all word index in dict\n",
          "    output:\n",
          "    1. B - omiision matrix in size of (amount of tags)X(amounts of words)\n",
          "    \"\"\"\n",
          "    B = np.zeros([len(unique_pos), len(words_indexing_dict)])\n",
          "    B_row_index = 0\n",
          "    for i_pos in unique_pos:\n",
          "        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
          "        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
          "        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
          "        for i_word in i_pos_words:\n",
          "            updated_percent_per_word_per_pos = i_pos_percent[np.where(i_pos_words == i_word)[0][0]]\n",
          "            B_column_word_index = words_indexing_dict[i_word]\n",
          "            B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
          "        B_row_index += 1\n",
          "\n",
          "    return B\n",
          "\n",
          "def generate_transition_matrix_A_and_pi(train_df, unique_pos, pos_indexing_dict):\n",
          "    \"\"\"\n",
          "    input\n",
          "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
          "     2. unique_pos - all tags base train data\n",
          "     3. pos_indexing_dict - all tags index in dict\n",
          "    output:\n",
          "    1. A - transition matrix matrix in size of (amount of tags)X(amount of tags)\n",
          "    2. Pi  - initial matrix, initilized with uniform probabiliies sized (amount of tags)X(1)\n",
          "    \"\"\"\n",
          "    A = np.zeros((len(unique_pos), len(unique_pos)))\n",
          "    pi = np.zeros(([len(unique_pos), 1]))\n",
          "\n",
          "    sentence_ind = 1\n",
          "    pi[pos_indexing_dict[train_df.iloc[0, :]['p']]] += 1\n",
          "\n",
          "    for i in range(1, train_df.shape[0]):\n",
          "        curr_sentence_ind = train_df.loc[i, 's']\n",
          "        if curr_sentence_ind != sentence_ind:\n",
          "            pi[pos_indexing_dict[train_df.loc[i, 'p']]] += 1\n",
          "            sentence_ind += 1\n",
          "        else:\n",
          "            A[pos_indexing_dict[train_df.loc[i-1, 'p']], pos_indexing_dict[train_df.loc[i, 'p']]] += 1\n",
          "    \n",
          "    A = A/A.sum(axis=1, keepdims=True)\n",
          "    pi = pi / sum(pi)\n",
          "\n",
          "    return A, pi\n",
          "\n",
          "# def generate_transition_matrix_A(train_df, unique_pos, unique_words):\n",
          "#     unique_words_list  = unique_words.tolist()\n",
          "#     unique_pos_list  = unique_pos.tolist()\n",
          "#     A = np.zeros([len(unique_pos), len(unique_pos)])\n",
          "#     rol_train_df = train_df.copy()\n",
          "#     rol_train_df = rol_train_df.iloc[np.arange(-1, len(rol_train_df) - 1)].reset_index(drop=True)\n",
          "#     rol_train_df = rol_train_df[1:]\n",
          "#     A_row_index = 0\n",
          "#     for i_pos in unique_pos:\n",
          "#         index_2_slice = (train_df['p'] == i_pos)\n",
          "#         index_2_slice = index_2_slice.iloc[np.arange(-1, len(index_2_slice) - 1)].reset_index(drop=True)\n",
          "#         index_2_slice.iloc[0] = False\n",
          "#         i_pos_train_df = train_df.loc[index_2_slice]\n",
          "\n",
          "#         i_pos_pos, i_pos_pos_count = np.unique(i_pos_train_df.loc[:, 'p'].values, return_counts=True)\n",
          "#         i_pos_percent = i_pos_pos_count / np.sum(i_pos_pos_count)\n",
          "#         i_pos_pos_list = i_pos_pos.tolist()\n",
          "#         for i_pos_next in i_pos_pos_list:\n",
          "#             if i_pos_next in i_pos_pos_list:\n",
          "#                 pos_index = i_pos_pos_list.index(i_pos_next)\n",
          "#                 updated_percent_per_pos_per_pos = i_pos_percent[pos_index]\n",
          "#                 A_column_pos_index = unique_pos_list.index(i_pos_next)\n",
          "#                 A[A_row_index, A_column_pos_index] = updated_percent_per_pos_per_pos\n",
          "#             else:\n",
          "#                 continue\n",
          "#         A_row_index += 1\n",
          "#     return A\n",
          "\n",
          "\n",
          "# def extract_ommision_matrix_B_2(train_df, unique_pos, unique_words):\n",
          "#     unique_words_list = unique_words.tolist()\n",
          "#     unique_pos_list = unique_pos.tolist()\n",
          "\n",
          "#     B = np.zeros([len(unique_pos), len(unique_words)])\n",
          "#     B_row_index = 0\n",
          "#     for i_pos in unique_pos:\n",
          "#         i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
          "#         i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
          "#         i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
          "#         i_pos_words_list = i_pos_words.tolist()\n",
          "#         for i_word in i_pos_words_list:\n",
          "#             if i_word in i_pos_words_list:\n",
          "#                 word_index = i_pos_words_list.index(i_word)\n",
          "#                 updated_percent_per_word_per_pos = i_pos_percent[word_index]\n",
          "#                 B_column_word_index = unique_words_list.index(i_word)\n",
          "#                 B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
          "#             else:\n",
          "#                 continue\n",
          "#         B_row_index += 1\n",
          "\n",
          "#     return B"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {
          "id": "q0Z9BMNM7EP3"
        },
        "source": [
          "**Part 2**\n",
          "\n",
          "Write a class simple_tagger, with methods train and evaluate. The method train receives the data as a list of sentences, and use it for training the tagger. In this case, it should learn a simple dictionary that maps words to tags, defined as the most frequent tag for every word (in case there is more than one most frequent tag, you may select one of them randomly). The dictionary should be stored as a class member for evaluation.\n",
          "\n",
          "The method evaluate receives the data as a list of sentences, and use it to evaluate the tagger performance. Specifically, you should calculate the word and sentence level accuracy. The evaluation process is simply going word by word, querying the dictionary (created by the train method) for each word’s tag and compare it to the true tag of that word. The word-level accuracy is the number of successes divided by the number of words. For OOV (out of vocabulary, or unknown) words, the tagger should assign the most frequent tag in the entire training set (i.e., the mode). The function should return the two numbers: word level accuracy and sentence level accuracy."
        ]
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "def extract_ommision_matrix_B(train_df, unique_pos, words_indexing_dict):\n",
        "    \"\"\"\n",
        "    input\n",
        "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
        "     2. unique_pos - all tags base train data\n",
        "     3. words_indexing_dict - all word index in dict\n",
        "    output:\n",
        "    1. B - omiision matrix in size of (amount of tags)X(amounts of words)\n",
        "    \"\"\"\n",
        "    B = np.zeros([len(unique_pos), len(words_indexing_dict)])\n",
        "    B_row_index = 0\n",
        "    for i_pos in unique_pos:\n",
        "        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
        "        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
        "        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
        "        for i_word in i_pos_words:\n",
        "            updated_percent_per_word_per_pos = i_pos_percent[np.where(i_pos_words == i_word)[0][0]]\n",
        "            B_column_word_index = words_indexing_dict[i_word]\n",
        "            B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
        "        B_row_index += 1\n",
        "\n",
        "    return B\n",
        "\n",
        "def generate_transition_matrix_A_and_pi(train_df, unique_pos, pos_indexing_dict):\n",
        "    \"\"\"\n",
        "    input\n",
        "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
        "     2. unique_pos - all tags base train data\n",
        "     3. pos_indexing_dict - all tags index in dict\n",
        "    output:\n",
        "    1. A - transition matrix matrix in size of (amount of tags)X(amount of tags)\n",
        "    2. Pi  - initial matrix, initilized with uniform probabiliies sized (amount of tags)X(1)\n",
        "    \"\"\"\n",
        "    A = np.zeros((len(unique_pos), len(unique_pos)))\n",
        "    pi = np.zeros(([len(unique_pos), 1]))\n",
        "\n",
        "    sentence_ind = 1\n",
        "    pi[pos_indexing_dict[train_df.iloc[0, :]['p']]] += 1\n",
        "\n",
        "    for i in range(1, train_df.shape[0]):\n",
        "        curr_sentence_ind = train_df.loc[i, 's']\n",
        "        if curr_sentence_ind != sentence_ind:\n",
        "            pi[pos_indexing_dict[train_df.loc[i, 'p']]] += 1\n",
        "            sentence_ind += 1\n",
        "        else:\n",
        "            A[pos_indexing_dict[train_df.loc[i-1, 'p']], pos_indexing_dict[train_df.loc[i, 'p']]] += 1\n",
        "    \n",
        "    A = A/A.sum(axis=1, keepdims=True)\n",
        "    pi = pi / sum(pi)\n",
        "\n",
        "    return A, pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Z9BMNM7EP3"
=======
      {
        "cell_type": "code",
        "execution_count": 9,
        "metadata": {
          "id": "GmNW2n6AsCYO"
        },
        "outputs": [],
        "source": [
          "def get_list_of_sentences_tag_lists(df):\n",
          "    \"\"\"\n",
          "    input\n",
          "     1. df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
          "    output:\n",
          "    1. sentence_list  - list of sentence inside list of word for each sentence \n",
          "    2. sentence_tag_list - list of sentence inside list of tag for each sentence \n",
          "    \"\"\"\n",
          "    df['i'] = pd.to_numeric(df['i'], errors='coerce')\n",
          "    df['s'] = pd.to_numeric(df['s'], errors='coerce')\n",
          "\n",
          "    word_index_array = df['i'].to_numpy()\n",
          "    initial_sentence_idx = np.where(word_index_array == 1)[0]\n",
          "    words_list = df['w'].to_list()\n",
          "    tag_list = df['p'].to_list()\n",
          "\n",
          "    sentence_list  = (initial_sentence_idx.size)*['None']\n",
          "    sentence_tag_list  = (initial_sentence_idx.size)*['None']\n",
          "\n",
          "    for sentence_idx in range(initial_sentence_idx.size):\n",
          "        if not sentence_idx == initial_sentence_idx.size-1:\n",
          "            \n",
          "            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]\n",
          "            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]\n",
          "        else:\n",
          "            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]::]\n",
          "            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]::]\n",
          "        \n",
          "        sentence_list[sentence_idx] = curr_sentence\n",
          "        sentence_tag_list[sentence_idx] = curr_tag\n",
          "\n",
          "    return sentence_list, sentence_tag_list"
        ]
>>>>>>> e357d52e6f4f870063cf04ed0b423a11112c199f
      },
      {
        "cell_type": "code",
        "execution_count": 10,
        "metadata": {
          "id": "MtivZLBH7dXq"
        },
        "outputs": [],
        "source": [
          "class simple_tagger:\n",
          "    def __init__(self):\n",
          "        self.tagger = {}\n",
          "    \n",
          "    def train(self, data):\n",
          "        \"\"\"\n",
          "        for each word give tag probabily base train data \n",
          "        loop on each word in word bank and give his tag base most fequente apperance \n",
          "        \"\"\"\n",
          "        pos = np.unique(data['p'], return_counts=True)\n",
          "        most_frequent_pos = pos[0][np.where(pos[1] == max(pos[1]))][0]\n",
          "\n",
          "        tagger = defaultdict(lambda: most_frequent_pos)\n",
          "\n",
          "        unique_words = np.unique(data.loc[:, 'w'].values)\n",
          "        for word in unique_words:\n",
          "            word_data = data.loc[data['w'] == word]\n",
          "            word_data_count = word_data['p'].value_counts()\n",
          "            predicted_tag = word_data_count.index[0]\n",
          "            tagger[word] = predicted_tag\n",
          "\n",
          "        self.tagger = tagger\n",
          "    def evaluate(self, data):\n",
          "        \"\"\"\n",
          "        for given data evaluate out tagging abilities base simple tagger\n",
          "        first loop run on sentences\n",
          "            secound loop run each word&tag of sentence\n",
          "        \"\"\"\n",
          "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
          "\n",
          "        words_success = 0\n",
          "        sentences_success = 0\n",
          "        idx = pd.IndexSlice\n",
          "        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
          "            count_successes = 0\n",
          "            for (word, actual_tag) in zip(sentence, sentence_tags):\n",
          "                predicted_tag = self.tagger[word]\n",
          "\n",
          "                if predicted_tag == actual_tag:\n",
          "                    words_success += 1\n",
          "                    count_successes += 1\n",
          "\n",
          "            if count_successes == len(sentence):\n",
          "                sentences_success += 1\n",
          "\n",
          "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
          "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
          "\n",
          "        return word_accuracy, sentence_accuracy"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 11,
        "metadata": {},
        "outputs": [],
        "source": [
          "tagger = simple_tagger()\n",
          "tagger.train(train_df)"
        ]
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "class simple_tagger:\n",
        "    def __init__(self):\n",
        "        self.tagger = {}\n",
        "    \n",
        "    def train(self, data):\n",
        "        \"\"\"\n",
        "        for each word give tag probabily base train data \n",
        "        loop on each word in word bank and give his tag base most fequente apperance \n",
        "        \"\"\"\n",
        "        pos = np.unique(data['p'], return_counts=True)\n",
        "        most_frequent_pos = pos[0][np.where(pos[1] == max(pos[1]))][0]\n",
        "\n",
        "        tagger = defaultdict(lambda: most_frequent_pos)\n",
        "\n",
        "        unique_words = np.unique(data.loc[:, 'w'].values)\n",
        "        for word in unique_words:\n",
        "            word_data = data.loc[data['w'] == word]\n",
        "            word_data_count = word_data['p'].value_counts()\n",
        "            predicted_tag = word_data_count.index[0]\n",
        "            tagger[word] = predicted_tag\n",
        "\n",
        "        self.tagger = tagger\n",
        "    def evaluate(self, data):\n",
        "        \"\"\"\n",
        "        for given data evaluate out tagging abilities base simple tagger\n",
        "        first loop run on sentences\n",
        "            secound loop run each word&tag of sentence\n",
        "        \"\"\"\n",
        "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
        "\n",
        "        words_success = 0\n",
        "        sentences_success = 0\n",
        "        for (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
        "            count_successes = 0\n",
        "            for (word, actual_tag) in zip(sentence, sentence_tags):\n",
        "                predicted_tag = self.tagger[word]\n",
        "\n",
        "                if predicted_tag == actual_tag:\n",
        "                    words_success += 1\n",
        "                    count_successes += 1\n",
        "\n",
        "            if count_successes == len(sentence):\n",
        "                sentences_success += 1\n",
        "\n",
        "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
        "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
        "\n",
        "        return word_accuracy, sentence_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "tagger = simple_tagger()\n",
        "tagger.train(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Rg0O7iYU3V1N"
=======
      {
        "cell_type": "code",
        "execution_count": 12,
        "metadata": {
          "id": "Rg0O7iYU3V1N"
        },
        "outputs": [],
        "source": [
          "simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train = tagger.evaluate(train_df)\n",
          "simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev = tagger.evaluate(dev_df)\n",
          "simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test = tagger.evaluate(test_df)"
        ]
>>>>>>> e357d52e6f4f870063cf04ed0b423a11112c199f
      },
      "execution_count": 7,
      "metadata": {},
      "output_type": "execute_result"
    }
  ],
  "source": [
    "train_df"
  ]
},
{
  "cell_type": "code",
  "execution_count": 8,
  "metadata": {
    "id": "Tfqqluah3V1H"
  },
  "outputs": [],
  "source": [
    "def extract_ommision_matrix_B(train_df, unique_pos, words_indexing_dict):\n",
    "    \"\"\"\n",
    "    input\n",
    "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
    "     2. unique_pos - all tags base train data\n",
    "     3. words_indexing_dict - all word index in dict\n",
    "    output:\n",
    "    1. B - omiision matrix in size of (amount of tags)X(amounts of words)\n",
    "    \"\"\"\n",
    "    B = np.zeros([len(unique_pos), len(words_indexing_dict)])\n",
    "    B_row_index = 0\n",
    "    for i_pos in unique_pos:\n",
    "        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
    "        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
    "        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
    "        for i_word in i_pos_words:\n",
    "            updated_percent_per_word_per_pos = i_pos_percent[np.where(i_pos_words == i_word)[0][0]]\n",
    "            B_column_word_index = words_indexing_dict[i_word]\n",
    "            B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
    "        B_row_index += 1\n",
    "\n",
    "    return B\n",
    "\n",
    "def generate_transition_matrix_A_and_pi(train_df, unique_pos, pos_indexing_dict):\n",
    "    \"\"\"\n",
    "    input\n",
    "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
    "     2. unique_pos - all tags base train data\n",
    "     3. pos_indexing_dict - all tags index in dict\n",
    "    output:\n",
    "    1. A - transition matrix matrix in size of (amount of tags)X(amount of tags)\n",
    "    2. Pi  - initial matrix, initilized with uniform probabiliies sized (amount of tags)X(1)\n",
    "    \"\"\"\n",
    "    A = np.zeros((len(unique_pos), len(unique_pos)))\n",
    "    pi = np.zeros(([len(unique_pos), 1]))\n",
    "\n",
    "    sentence_ind = 1\n",
    "    pi[pos_indexing_dict[train_df.iloc[0, :]['p']]] += 1\n",
    "\n",
    "    for i in range(1, train_df.shape[0]):\n",
    "        curr_sentence_ind = train_df.index[i][0]\n",
    "        if curr_sentence_ind != sentence_ind:\n",
    "            pi[pos_indexing_dict[train_df.iloc[i, :]['p']]] += 1\n",
    "            sentence_ind += 1\n",
    "        else:\n",
    "            A[pos_indexing_dict[train_df.iloc[i-1, :]['p']], pos_indexing_dict[train_df.iloc[i, :]['p']]] += 1\n",
    "    \n",
    "    A = A/A.sum(axis=1, keepdims=True)\n",
    "    pi = pi / sum(pi)\n",
    "    \n",
    "    return A, pi\n",
    "\n",
    "def generate_transition_matrix_A(train_df, unique_pos, unique_words):\n",
    "    \"\"\"\n",
    "    input\n",
    "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
    "     2. unique_pos - all tags base train data\n",
    "     3. unique_words - all words base train data\n",
    "    output:\n",
    "    1. A - transition matrix matrix in size of (amount of tags)X(amount of tags)\n",
    "    \"\"\"\n",
    "    unique_words_list  = unique_words.tolist()\n",
    "    unique_pos_list  = unique_pos.tolist()\n",
    "    A = np.zeros([len(unique_pos), len(unique_pos)])\n",
    "    rol_train_df = train_df.copy()\n",
    "    rol_train_df = rol_train_df.iloc[np.arange(-1, len(rol_train_df) - 1)].reset_index(drop=True)\n",
    "    rol_train_df = rol_train_df[1:]\n",
    "    A_row_index = 0\n",
    "    for i_pos in unique_pos:\n",
    "        index_2_slice = (train_df['p'] == i_pos)\n",
    "        index_2_slice = index_2_slice.iloc[np.arange(-1, len(index_2_slice) - 1)].reset_index(drop=True)\n",
    "        index_2_slice.iloc[0] = False\n",
    "        i_pos_train_df = train_df.loc[index_2_slice]\n",
    "\n",
    "        i_pos_pos, i_pos_pos_count = np.unique(i_pos_train_df.loc[:, 'p'].values, return_counts=True)\n",
    "        i_pos_percent = i_pos_pos_count / np.sum(i_pos_pos_count)\n",
    "        i_pos_pos_list = i_pos_pos.tolist()\n",
    "        for i_pos_next in i_pos_pos_list:\n",
    "            if i_pos_next in i_pos_pos_list:\n",
    "                pos_index = i_pos_pos_list.index(i_pos_next)\n",
    "                updated_percent_per_pos_per_pos = i_pos_percent[pos_index]\n",
    "                A_column_pos_index = unique_pos_list.index(i_pos_next)\n",
    "                A[A_row_index, A_column_pos_index] = updated_percent_per_pos_per_pos\n",
    "            else:\n",
    "                continue\n",
    "        A_row_index += 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def extract_ommision_matrix_B_2(train_df, unique_pos, unique_words):\n",
    "    \"\"\"\n",
    "    input\n",
    "     1. train_df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
    "     2. unique_pos - all tags base train data\n",
    "     3. unique_words - all words base train data\n",
    "    output:\n",
    "    1. B - omiision matrix in size of (amount of tags)X(amounts of words)\n",
    "    \"\"\"\n",
    "    unique_words_list = unique_words.tolist()\n",
    "    unique_pos_list = unique_pos.tolist()\n",
    "\n",
    "    B = np.zeros([len(unique_pos), len(unique_words)])\n",
    "    B_row_index = 0\n",
    "    for i_pos in unique_pos:\n",
    "        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]\n",
    "        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)\n",
    "        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)\n",
    "        i_pos_words_list = i_pos_words.tolist()\n",
    "        for i_word in i_pos_words_list:\n",
    "            if i_word in i_pos_words_list:\n",
    "                word_index = i_pos_words_list.index(i_word)\n",
    "                updated_percent_per_word_per_pos = i_pos_percent[word_index]\n",
    "                B_column_word_index = unique_words_list.index(i_word)\n",
    "                B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos\n",
    "            else:\n",
    "                continue\n",
    "        B_row_index += 1\n",
    "\n",
    "    return B"
  ]
},
{
  "cell_type": "markdown",
  "metadata": {
    "id": "q0Z9BMNM7EP3"
  },
  "source": [
    "**Part 2**\n",
    "\n",
    "Write a class simple_tagger, with methods train and evaluate. The method train receives the data as a list of sentences, and use it for training the tagger. In this case, it should learn a simple dictionary that maps words to tags, defined as the most frequent tag for every word (in case there is more than one most frequent tag, you may select one of them randomly). The dictionary should be stored as a class member for evaluation.\n",
    "\n",
    "The method evaluate receives the data as a list of sentences, and use it to evaluate the tagger performance. Specifically, you should calculate the word and sentence level accuracy. The evaluation process is simply going word by word, querying the dictionary (created by the train method) for each word’s tag and compare it to the true tag of that word. The word-level accuracy is the number of successes divided by the number of words. For OOV (out of vocabulary, or unknown) words, the tagger should assign the most frequent tag in the entire training set (i.e., the mode). The function should return the two numbers: word level accuracy and sentence level accuracy."
  ]
},
{
  "cell_type": "code",
  "execution_count": 9,
  "metadata": {
    "id": "GmNW2n6AsCYO"
  },
  "outputs": [],
  "source": [
    "def get_list_of_sentences_tag_lists(df):\n",
    "    \"\"\"\n",
    "    input\n",
    "     1. df - data frame to train with contrain  = ['sentence index', 'work index in sentence', 'word','tag']\n",
    "    output:\n",
    "    1. sentence_list  - list of sentence inside list of word for each sentence \n",
    "    2. sentence_tag_list - list of sentence inside list of tag for each sentence \n",
    "    \"\"\"\n",
    "    \n",
    "    df['i'] = pd.to_numeric(df['i'], errors='coerce')\n",
    "    df['s'] = pd.to_numeric(df['s'], errors='coerce')\n",
    "\n",
    "    word_index_array = df['i'].to_numpy()\n",
    "    initial_sentence_idx = np.where(word_index_array == 1)[0]\n",
    "    words_list = df['w'].to_list()\n",
    "    tag_list = df['p'].to_list()\n",
    "\n",
    "    sentence_list  = (initial_sentence_idx.size)*['None']\n",
    "    sentence_tag_list  = (initial_sentence_idx.size)*['None']\n",
    "\n",
    "    for sentence_idx in range(initial_sentence_idx.size):\n",
    "        if not sentence_idx == initial_sentence_idx.size-1:\n",
    "            \n",
    "            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]\n",
    "            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]\n",
    "        else:\n",
    "            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]::]\n",
    "            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]::]\n",
    "        \n",
    "        sentence_list[sentence_idx] = curr_sentence\n",
    "        sentence_tag_list[sentence_idx] = curr_tag\n",
    "\n",
    "    return sentence_list, sentence_tag_list"
  ]
},
{
  "cell_type": "code",
  "execution_count": 10,
  "metadata": {
    "id": "MtivZLBH7dXq"
  },
  "outputs": [],
  "source": [
    "class simple_tagger:\n",
    "    def __init__(self):\n",
    "        self.tagger = {}\n",
    "    \n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "        for each word give tag probabily base train data \n",
    "        loop on each word in word bank and give his tag base most fequente apperance \n",
    "        \"\"\"\n",
    "        pos = np.unique(data['p'], return_counts=True)\n",
    "        most_frequent_pos = pos[0][np.where(pos[1] == max(pos[1]))][0]\n",
    "\n",
    "        tagger = defaultdict(lambda: most_frequent_pos)\n",
    "\n",
    "        unique_words = np.unique(data.loc[:, 'w'].values)\n",
    "        for word in unique_words:\n",
    "            word_data = data.loc[data['w'] == word]\n",
    "            word_data_count = word_data['p'].value_counts()\n",
    "            predicted_tag = word_data_count.index[0]\n",
    "            tagger[word] = predicted_tag\n",
    "\n",
    "        self.tagger = tagger\n",
    "    def evaluate(self, data):\n",
    "        \"\"\"\n",
    "        for given data evaluate out tagging abilities base simple tagger\n",
    "        first loop run on sentences\n",
    "            secound loop run each word&tag of sentence\n",
    "        \"\"\"\n",
    "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
    "        words_success = 0\n",
    "        sentences_success = 0\n",
    "        idx = pd.IndexSlice\n",
    "        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
    "            count_successes = 0\n",
    "            for (word, actual_tag) in zip(sentence, sentence_tags):\n",
    "                predicted_tag = self.tagger[word]\n",
    "\n",
    "                if predicted_tag == actual_tag:\n",
    "                    words_success += 1\n",
    "                    count_successes += 1\n",
    "\n",
    "            if count_successes == len(sentence)-1:\n",
    "                sentences_success += 1\n",
    "\n",
    "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
    "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
    "\n",
    "        return word_accuracy, sentence_accuracy"
  ]
},
{
  "cell_type": "code",
  "execution_count": 11,
  "metadata": {},
  "outputs": [],
  "source": [
    "tagger = simple_tagger()\n",
    "tagger.train(train_df)"
  ]
},
{
  "cell_type": "code",
  "execution_count": 12,
  "metadata": {
    "id": "Rg0O7iYU3V1N"
  },
  "outputs": [],
  "source": [
    "simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train = tagger.evaluate(train_df)\n",
    "simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev = tagger.evaluate(dev_df)\n",
    "simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test = tagger.evaluate(test_df)"
  ]
},
{
  "cell_type": "code",
  "execution_count": 13,
  "metadata": {
    "colab": {
      "base_uri": "https://localhost:8080/",
      "height": 143
    },
    "id": "STiEtEUK3V1N",
    "outputId": "cb899ccf-144f-4747-d409-9736f60e0428"
  },
  "outputs": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "STiEtEUK3V1N",
        "outputId": "cb899ccf-144f-4747-d409-9736f60e0428"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data-set</th>\n",
              "      <th>word-accuracy[%]</th>\n",
              "      <th>sentence-accuracy[%]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>93.0922</td>\n",
              "      <td>36.7491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dev</td>\n",
              "      <td>84.8731</td>\n",
              "      <td>16.3701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>82.5907</td>\n",
              "      <td>16.2192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  data-set  word-accuracy[%]  sentence-accuracy[%]\n",
              "0    train           93.0922               36.7491\n",
              "1      dev           84.8731               16.3701\n",
              "2     test           82.5907               16.2192"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_list = [['train' ,simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train],\n",
        "['dev', simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev],\n",
        "['test', simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test]]\n",
        "\n",
        "simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
        "simple_tagger_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK9iZIq8i0X"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Similar to part 2, write the class hmm_tagger, which implements HMM tagging. The method *train* should build the matrices A, B and Pi, from the data as discussed in class. The method *evaluate* should find the best tag sequence for every input sentence using he Viterbi decoding algorithm, and then calculate the word and sentence level accuracy using the gold-standard tags. You should implement the Viterbi algorithm in the next block and call it from your class.\n",
        "\n",
        "Additional guidance:\n",
        "1. The matrix B represents the emissions probabilities. Since B is a matrix, you should build a dictionary that maps every unique word in the corpus to a serial numeric id (starting with 0). This way columns in B represents word ids.\n",
        "2. During the evaluation, you should first convert each word into it’s index and then create the observation array to be given to Viterbi, as a list of ids. OOV words should be assigned with a random tag. To make sure Viterbi works appropriately, you can simply break the sentence into multiple segments every time you see an OOV word, and decode every segment individually using Viterbi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BYMC--2xsCYR"
      },
      "outputs": [],
      "source": [
        "def dptable(V):\n",
        "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
        "    for state in V[0]:\n",
        "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)\n",
        "\n",
        "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
        "    \"\"\"\n",
        "     input\n",
        "     1. observations - sentence word list\n",
        "     2. states - all availiable states(words)\n",
        "     3. start_p - initial pie matrxi \n",
        "     4. trans_p - transition matrix\n",
        "     5. emit_p  - omission matrix \n",
        "     \n",
        "     output:\n",
        "     1. opt  - list of tags per input sentence\n",
        "    \"\"\"\n",
        "    V = [{}]\n",
        "    for st in states:\n",
        "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
        "   \n",
        "    for t in range(1, len(observations)):\n",
        "        V.append({})\n",
        "        for st in states:\n",
        "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
        "            prev_st_selected = states[0]\n",
        "            for prev_st in states[1:]:\n",
        "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
        "                if tr_prob > max_tr_prob:\n",
        "                    max_tr_prob = tr_prob\n",
        "                    prev_st_selected = prev_st\n",
        " \n",
        "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
        "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
        "     \n",
        "     # for line in dptable(V):\n",
        "     #    print(line)\n",
        " \n",
        "    opt = []\n",
        "    max_prob = 0.0\n",
        "    best_st = None\n",
        " \n",
        "    for st, data in V[-1].items():\n",
        "        if data[\"prob\"] > max_prob:\n",
        "            max_prob = data[\"prob\"]\n",
        "            best_st = st\n",
        "     \n",
        "    if best_st == None:\n",
        "         best_st = random.choice(list(V[-1].keys()))\n",
        "    opt.append(best_st)\n",
        "    previous = best_st\n",
        " \n",
        " \n",
        "    for t in range(len(V) - 2, -1, -1):\n",
        "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
        "        previous = V[t + 1][previous][\"prev\"]\n",
        "        \n",
        "    return opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TpH7GuiQ9L6W"
      },
      "outputs": [],
      "source": [
        "class hmm_tagger:\n",
        "    data = pd.DataFrame()\n",
        "    A = []\n",
        "    B = []\n",
        "    Pi = []\n",
        "    uniqueWordsArr = []\n",
        "    \n",
        "    def __init__(self, data_df):\n",
        "\n",
        "        pos_values = list(np.unique(data_df.loc[:, 'p'].values, return_counts=True))\n",
        "        unique_words = np.unique(data_df.loc[:, 'w'].values)\n",
        "        unique_pos = pos_values[0]\n",
        "\n",
        "        words_indexing_dict = {unique_words[i] : i for i in range(len(unique_words))}\n",
        "        pos_indexing_dict = {unique_pos[i] : i for i in range(len(unique_pos))}\n",
        "\n",
        "        unique_words_list  = unique_words.tolist()\n",
        "        unique_pos_list  = unique_pos.tolist()\n",
        "        \n",
        "        self.data_df = data_df\n",
        "        self.unique_pos = unique_pos\n",
        "        self.unique_words = unique_words\n",
        "        self.unique_words_list = unique_words_list\n",
        "        self.unique_pos_list = unique_pos_list\n",
        "        self.words_indexing_dict = words_indexing_dict\n",
        "        self.pos_indexing_dict = pos_indexing_dict\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        for given data\n",
        "        generate ommision matrix, transition matrix, pi initial matrix\n",
        "        \"\"\"\n",
        "        data_df = self.data_df\n",
        "        unique_pos = self.unique_pos\n",
        "        unique_words = self.unique_words\n",
        "        \n",
        "        \n",
        "        B = extract_ommision_matrix_B(self.data_df, self.unique_pos, self.words_indexing_dict)\n",
        "        A, Pi = generate_transition_matrix_A_and_pi(self.data_df, self.unique_pos, self.pos_indexing_dict)\n",
        "        # Pi = np.ones([unique_pos.size,1])*(1/unique_pos.size)\n",
        "\n",
        "        B = pd.DataFrame(B.T, index = unique_words, columns = unique_pos)\n",
        "        A = pd.DataFrame(A.T, index =unique_pos , columns = unique_pos)\n",
        "        Pi = pd.DataFrame(Pi.T, columns = unique_pos)\n",
        "   \n",
        "        self.states = tuple(unique_pos)\n",
        "        self.A = A.to_dict()\n",
        "        self.B = B.to_dict()\n",
        "        self.Pi = Pi.to_dict('records')[0]\n",
        "        \n",
        "        # self.A, self.B, self.Pi = calc_A_B_Pi(data_df)\n",
        "        self.uniqueWordsArr = unique_words.tolist()\n",
        "\n",
        "    \n",
        "    def evaluate(self, data):\n",
        "        \"\"\"\n",
        "        for given data evaluate out tagging abilities base hmm tagger\n",
        "        first loop run on sentences\n",
        "            using viterbi_algorithm prdict the tag for each word in sentence \n",
        "            secound loop run each word&tag of sentence\n",
        "        \"\"\"\n",
        "        # sentences = sentences_from_df(data)\n",
        "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
        "        sentenceIdx = [] \n",
        "        words_success = 0\n",
        "        sentences_success = 0\n",
        "        idx = pd.IndexSlice\n",
        "        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
        "            # for sentence_num, sentence in enumerate(sentences, 1):\n",
        "            count_successes = 0\n",
        "            \n",
        "            \n",
        "            # validate all word in sentence is in train data set otherwise\n",
        "            # choose randomly word from bank of word\n",
        "            for i_word_idx, word in enumerate(sentence):\n",
        "                if not word in self.unique_words:\n",
        "                    sentence[i_word_idx] = random.choice(self.unique_words_list)\n",
        "                \n",
        "                \n",
        "            predicted_tag_list = viterbi_algorithm(tuple(sentence), self.states, self.Pi, self.A, self.B)  \n",
        "            \n",
        "            for word_num, (predicted_tag, actual_tag) in enumerate(zip(predicted_tag_list, sentence_tags)):\n",
        "                # print('word nun ' + str(word_num))\n",
        "\n",
        "                if predicted_tag == actual_tag:\n",
        "                    words_success += 1\n",
        "                    count_successes += 1\n",
        "\n",
        "            if count_successes == len(sentence):\n",
        "                sentences_success += 1\n",
        "\n",
        "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
        "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
        "\n",
        "        return word_accuracy, sentence_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DR6KJW2F9yqt"
      },
      "outputs": [],
      "source": [
        "hmmTagger = hmm_tagger(train_df)\n",
        "hmmTagger.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zvb6qUu6zHru"
      },
      "outputs": [],
      "source": [
        "hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train = hmmTagger.evaluate(train_df)\n",
        "hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test = hmmTagger.evaluate(test_df)\n",
        "hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev = hmmTagger.evaluate(dev_df)"
      ]
    },
    "execution_count": 13,
    "metadata": {},
    "output_type": "execute_result"
  }
],
"source": [
  "result_list = [['train' ,simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train],\n",
  "['dev', simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev],\n",
  "['test', simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test]]\n",
  "\n",
  "simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
  "simple_tagger_results.head()"
]
},
{
"cell_type": "markdown",
"metadata": {
  "id": "etK9iZIq8i0X"
},
"source": [
  "**Part 3**\n",
  "\n",
  "Similar to part 2, write the class hmm_tagger, which implements HMM tagging. The method *train* should build the matrices A, B and Pi, from the data as discussed in class. The method *evaluate* should find the best tag sequence for every input sentence using he Viterbi decoding algorithm, and then calculate the word and sentence level accuracy using the gold-standard tags. You should implement the Viterbi algorithm in the next block and call it from your class.\n",
  "\n",
  "Additional guidance:\n",
  "1. The matrix B represents the emissions probabilities. Since B is a matrix, you should build a dictionary that maps every unique word in the corpus to a serial numeric id (starting with 0). This way columns in B represents word ids.\n",
  "2. During the evaluation, you should first convert each word into it’s index and then create the observation array to be given to Viterbi, as a list of ids. OOV words should be assigned with a random tag. To make sure Viterbi works appropriately, you can simply break the sentence into multiple segments every time you see an OOV word, and decode every segment individually using Viterbi.\n"
]
},
{
"cell_type": "code",
"execution_count": 14,
"metadata": {
  "id": "BYMC--2xsCYR"
},
"outputs": [],
"source": [
  "def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):\n",
  "     \"\"\"\n",
  "     input\n",
  "     1. observations - sentence word list\n",
  "     2. states - all availiable states(words)\n",
  "     3. start_p - initial pie matrxi \n",
  "     4. trans_p - transition matrix\n",
  "     5. emit_p  - omission matrix \n",
  "     \n",
  "     output:\n",
  "     1. opt  - list of tags per input sentence\n",
  "     \"\"\"\n",
  "     # initilization step    \n",
  "     V = [{}]\n",
  "     for st in states:\n",
  "         V[0][st] = {\"prob\": start_p[st] * emit_p[st][observations[0]], \"prev\": None}\n",
  "     \n",
  "     # iteration step\n",
  "     for t in range(1, len(observations)):\n",
  "         V.append({})\n",
  "         for st in states:\n",
  "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
  "            prev_st_selected = states[0]\n",
  "            for prev_st in states[1:]:\n",
  "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
  "                if tr_prob > max_tr_prob:\n",
  "                    max_tr_prob = tr_prob\n",
  "                    prev_st_selected = prev_st\n",
  " \n",
  "            max_prob = max_tr_prob * emit_p[st][observations[t]]\n",
  "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
  " \n",
  "     opt = []\n",
  "     max_prob = 0.0\n",
  "     best_st = None\n",
  " \n",
  "     for st, data in V[-1].items():\n",
  "        if data[\"prob\"] > max_prob:\n",
  "            max_prob = data[\"prob\"]\n",
  "            best_st = st\n",
  "     \n",
  "     if best_st == None:\n",
  "         best_st = random.choice(list(V[-1].keys()))\n",
  "     opt.append(best_st)\n",
  "     previous = best_st\n",
  " \n",
  "     # seqyence recovery\n",
  "     for t in range(len(V) - 2, -1, -1):\n",
  "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
  "        previous = V[t + 1][previous][\"prev\"]\n",
  "        \n",
  "     return opt"
]
},
{
"cell_type": "code",
"execution_count": 15,
"metadata": {
  "id": "TpH7GuiQ9L6W"
},
"outputs": [],
"source": [
  "class hmm_tagger:\n",
  "    data = pd.DataFrame()\n",
  "    A = []\n",
  "    B = []\n",
  "    Pi = []\n",
  "    uniqueWordsArr = []\n",
  "    \n",
  "    def __init__(self, data_df):\n",
  "        \n",
  "        unique_pos, pos_counts = list(np.unique(data_df.loc[:, 'p'].values, return_counts=True))\n",
  "        unique_words, word_counts = np.unique(data_df.loc[:, 'w'].values, return_counts=True)\n",
  "        unique_words_list  = unique_words.tolist()\n",
  "        unique_pos_list  = unique_pos.tolist()\n",
  "        \n",
  "        self.data_df = data_df\n",
  "        self.unique_pos = unique_pos\n",
  "        self.unique_words = unique_words\n",
  "        self.unique_words_list = unique_words_list\n",
  "        self.unique_pos_list = unique_pos_list\n",
  "\n",
  "\n",
  "    \n",
  "    def train(self):\n",
  "        \"\"\"\n",
  "        for given data\n",
  "        generate ommision matrix, transition matrix, pi initial matrix\n",
  "        \"\"\"\n",
  "        data_df = self.data_df\n",
  "        unique_pos = self.unique_pos\n",
  "        unique_words = self.unique_words\n",
  "        \n",
  "        \n",
  "        B = extract_ommision_matrix_B_2(self.data_df, unique_pos, unique_words)\n",
  "        A = generate_transition_matrix_A(self.data_df, unique_pos, unique_words)\n",
  "        Pi = np.ones([unique_pos.size,1])*(1/unique_pos.size)\n",
  "\n",
  "        B = pd.DataFrame(B.T, index = unique_words, columns = unique_pos)\n",
  "        A = pd.DataFrame(A.T, index =unique_pos , columns = unique_pos)\n",
  "        Pi = pd.DataFrame(Pi.T, columns = unique_pos)\n",
  "   \n",
  "        self.states = tuple(unique_pos)\n",
  "        self.A = A.to_dict()\n",
  "        self.B = B.to_dict()\n",
  "        self.Pi = Pi.to_dict('records')[0]\n",
  "        \n",
  "        # self.A, self.B, self.Pi = calc_A_B_Pi(data_df)\n",
  "        self.uniqueWordsArr = unique_words.tolist()\n",
  "\n",
  "   \n",
  "    \n",
  "    def evaluate(self, data):\n",
  "        \"\"\"\n",
  "        for given data evaluate out tagging abilities base hmm tagger\n",
  "        first loop run on sentences\n",
  "            using viterbi_algorithm prdict the tag for each word in sentence \n",
  "            secound loop run each word&tag of sentence\n",
  "        \"\"\"\n",
  "        # sentences = sentences_from_df(data)\n",
  "        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)\n",
  "        sentenceIdx = [] \n",
  "        words_success = 0\n",
  "        sentences_success = 0\n",
  "        idx = pd.IndexSlice\n",
  "        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):\n",
  "            count_successes = 0\n",
  "            \n",
  "            # validate all word in sentence is in train data set otherwise\n",
  "            # choose randomly word from bank of word\n",
  "            for i_word_idx, word in enumerate(sentence):\n",
  "                if not word in self.unique_words:\n",
  "                    sentence[i_word_idx] = random.choice(self.unique_words_list)\n",
  "                \n",
  "            predicted_tag_list = viterbi_algorithm(tuple(sentence), self.states, self.Pi, self.A, self.B)  \n",
  "            \n",
  "            for word_num, (predicted_tag, actual_tag) in enumerate(zip(predicted_tag_list, sentence_tags)):\n",
  "\n",
  "                if predicted_tag == actual_tag:\n",
  "                    words_success += 1\n",
  "                    count_successes += 1\n",
  "\n",
  "            if count_successes == len(sentence)-1:\n",
  "                sentences_success += 1\n",
  "\n",
  "        word_accuracy = round(words_success / data.shape[0] * 100, 4)\n",
  "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
  "\n",
  "        return word_accuracy, sentence_accuracy\n"
]
},
{
"cell_type": "code",
"execution_count": 16,
"metadata": {
  "id": "DR6KJW2F9yqt"
},
"outputs": [],
"source": [
  "hmmTagger = hmm_tagger(train_df)\n",
  "hmmTagger.train()"
]
},
{
"cell_type": "code",
"execution_count": 17,
"metadata": {
  "id": "zvb6qUu6zHru"
},
"outputs": [],
"source": [
  "hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train = hmmTagger.evaluate(train_df)\n",
  "hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test = hmmTagger.evaluate(test_df)\n",
  "hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev = hmmTagger.evaluate(dev_df)"
]
},
{
"cell_type": "code",
"execution_count": 18,
"metadata": {
  "colab": {
    "base_uri": "https://localhost:8080/",
    "height": 143
  },
  "id": "5Q5oYLOFsCYR",
  "outputId": "ebfb2b8e-ba59-4586-f21c-29e5717e633f"
},
"outputs": [
  {
    "cell_type": "code",
    "execution_count": 18,
    "metadata": {
      "colab": {
        "base_uri": "https://localhost:8080/",
        "height": 143
      },
      "id": "5Q5oYLOFsCYR",
      "outputId": "ebfb2b8e-ba59-4586-f21c-29e5717e633f"
    },
    "outputs": [
      {
        "data": {
          "text/html": [
            "<div>\n",
            "<style scoped>\n",
            "    .dataframe tbody tr th:only-of-type {\n",
            "        vertical-align: middle;\n",
            "    }\n",
            "\n",
            "    .dataframe tbody tr th {\n",
            "        vertical-align: top;\n",
            "    }\n",
            "\n",
            "    .dataframe thead th {\n",
            "        text-align: right;\n",
            "    }\n",
            "</style>\n",
            "<table border=\"1\" class=\"dataframe\">\n",
            "  <thead>\n",
            "    <tr style=\"text-align: right;\">\n",
            "      <th></th>\n",
            "      <th>data-set</th>\n",
            "      <th>word-accuracy[%]</th>\n",
            "      <th>sentence-accuracy[%]</th>\n",
            "    </tr>\n",
            "  </thead>\n",
            "  <tbody>\n",
            "    <tr>\n",
            "      <th>0</th>\n",
            "      <td>train</td>\n",
            "      <td>97.2138</td>\n",
            "      <td>65.0353</td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <th>1</th>\n",
            "      <td>dev</td>\n",
            "      <td>83.5343</td>\n",
            "      <td>19.5730</td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <th>2</th>\n",
            "      <td>test</td>\n",
            "      <td>79.2502</td>\n",
            "      <td>18.6801</td>\n",
            "    </tr>\n",
            "  </tbody>\n",
            "</table>\n",
            "</div>"
          ],
          "text/plain": [
            "  data-set  word-accuracy[%]  sentence-accuracy[%]\n",
            "0    train           97.2138               65.0353\n",
            "1      dev           83.5343               19.5730\n",
            "2     test           79.2502               18.6801"
          ]
        },
<<<<<<< HEAD
        "id": "KYhtboJm_Iyx",
        "outputId": "9ca1098c-9f0f-45d4-9087-da7c03b2b141"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import tnt \n",
        "list_word_tag_train_sentences = create_word_tag_tuples(ud_train)\n",
        "list_word_tag_dev_sentences = create_word_tag_tuples(ud_dev)\n",
        "list_word_tag_test_sentences = create_word_tag_tuples(ud_test)\n",
        "\n",
        "tnt_pos_tagger = tnt.TnT()\n",
        "tnt_pos_tagger.train(list_word_tag_train_sentences)\n",
        "\n",
        "memm_tagger_word_accuracy_train = round(tnt_pos_tagger.evaluate(list_word_tag_train_sentences) * 100, 4)\n",
        "memm_tagger_word_accuracy_test = round(tnt_pos_tagger.evaluate(list_word_tag_dev_sentences) * 100, 4)\n",
        "memm_tagger_word_accuracy_dev = round(tnt_pos_tagger.evaluate(list_word_tag_test_sentences) * 100, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_memm(data, tagger):\n",
        "        sentences_list = create_word_tag_tuples(data)\n",
        "\n",
        "        sentences_success = 0\n",
        "        for sentence in sentences_list:\n",
        "            count_successes = 0\n",
        "            predicted_tags = tagger.tag([word[0] for word in sentence])\n",
        "            for (_, actual_tag), predicted_tag in zip(sentence, predicted_tags):\n",
        "                if predicted_tag[1] == actual_tag:\n",
        "                    count_successes += 1\n",
        "\n",
        "            if count_successes == len(sentence):\n",
        "                sentences_success += 1\n",
        "\n",
        "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
        "\n",
        "        return sentence_accuracy"
      ]
=======
        "execution_count": 18,
        "metadata": {},
        "output_type": "execute_result"
      }
    ],
    "source": [
      "result_list = [['train' ,hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train],\n",
      "['dev', hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev],\n",
      "['test', hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test]]\n",
      "\n",
      "simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
      "simple_tagger_results.head()"
    ]
  },
  {
    "cell_type": "markdown",
    "metadata": {
      "id": "-YZO0uGL-4S-"
>>>>>>> e357d52e6f4f870063cf04ed0b423a11112c199f
    },
    "source": [
      "**Part 4**\n",
      "\n",
      "Compare the results obtained from both taggers and a MEMM tagger, implemented by NLTK (a known NLP library), over both, the dev and test datasets. To train the NLTK MEMM tagger you should execute the following lines (it may take some time to train...):"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 19,
    "metadata": {},
    "outputs": [],
    "source": [
      "def create_word_tag_tuples(file_path):\n",
      "  \"\"\"\n",
      "  input \n",
      "      1. file_path - conullu file path\n",
      "  outout\n",
      "      2. sentences  - senstence list\n",
      "  \"\"\"  \n",
      "  sentences = []\n",
      "  data_file = open(file_path, \"r\", encoding=\"utf-8\")\n",
      "  for tokenlist in parse_incr(data_file):\n",
      "      sentence = [(str(token['form']), str(token['xpos'])) for token in tokenlist]\n",
      "      sentences.append(sentence)  \n",
      "      \n",
      "  return sentences"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 20,
    "metadata": {},
    "outputs": [],
    "source": [
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\")"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 21,
    "metadata": {
      "colab": {
        "base_uri": "https://localhost:8080/",
        "height": 352
      },
      "id": "KYhtboJm_Iyx",
      "outputId": "9ca1098c-9f0f-45d4-9087-da7c03b2b141"
    },
    "outputs": [],
    "source": [
      "from nltk.tag import tnt \n",
      "list_word_tag_train_sentences = create_word_tag_tuples(ud_train)\n",
      "list_word_tag_dev_sentences = create_word_tag_tuples(ud_dev)\n",
      "list_word_tag_test_sentences = create_word_tag_tuples(ud_test)\n",
      "\n",
      "tnt_pos_tagger = tnt.TnT()\n",
      "tnt_pos_tagger.train(list_word_tag_train_sentences)\n",
      "\n",
      "memm_tagger_word_accuracy_train = round(tnt_pos_tagger.evaluate(list_word_tag_train_sentences) * 100, 4)\n",
      "memm_tagger_word_accuracy_test = round(tnt_pos_tagger.evaluate(list_word_tag_dev_sentences) * 100, 4)\n",
      "memm_tagger_word_accuracy_dev = round(tnt_pos_tagger.evaluate(list_word_tag_test_sentences) * 100, 4)"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 22,
    "metadata": {},
    "outputs": [],
    "source": [
      "def evaluate_memm(data, tagger):\n",
      "        sentences_list = create_word_tag_tuples(data)\n",
      "\n",
      "        sentences_success = 0\n",
      "        idx = pd.IndexSlice\n",
      "        for sentence in sentences_list:\n",
      "            count_successes = 0\n",
      "            predicted_tags = tagger.tag([word[0] for word in sentence])\n",
      "            for (_, actual_tag), predicted_tag in zip(sentence, predicted_tags):\n",
      "                if predicted_tag[1] == actual_tag:\n",
      "                    count_successes += 1\n",
      "\n",
      "            if count_successes == len(sentence):\n",
      "                sentences_success += 1\n",
      "\n",
      "        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
      "\n",
      "        return sentence_accuracy"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 23,
    "metadata": {},
    "outputs": [],
    "source": [
      "memm_tagger_sentence_accuracy_train = evaluate_memm(ud_train, tnt_pos_tagger)\n",
      "memm_tagger_sentence_accuracy_test = evaluate_memm(ud_test, tnt_pos_tagger)\n",
      "memm_tagger_sentence_accuracy_dev = evaluate_memm(ud_dev, tnt_pos_tagger)"
    ]
  },
  "execution_count": 18,
  "metadata": {},
  "output_type": "execute_result"
}
],
"source": [
"result_list = [['train' ,hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train],\n",
"['dev', hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev],\n",
"['test', hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test]]\n",
"\n",
"simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
"simple_tagger_results.head()"
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "-YZO0uGL-4S-"
},
"source": [
"**Part 4**\n",
"\n",
"Compare the results obtained from both taggers and a MEMM tagger, implemented by NLTK (a known NLP library), over both, the dev and test datasets. To train the NLTK MEMM tagger you should execute the following lines (it may take some time to train...):"
]
},
{
"cell_type": "code",
"execution_count": 19,
"metadata": {},
"outputs": [],
"source": [
"def create_word_tag_tuples(file_path):\n",
"  \"\"\"\n",
"  input \n",
"      1. file_path - conullu file path\n",
"  outout\n",
"      2. sentences  - senstence list\n",
"  \"\"\"  \n",
"  sentences = []\n",
"  data_file = open(file_path, \"r\", encoding=\"utf-8\")\n",
"  for tokenlist in parse_incr(data_file):\n",
"      sentence = [(str(token['form']), str(token['xpos'])) for token in tokenlist]\n",
"      sentences.append(sentence)  \n",
"      \n",
"  return sentences"
]
},
{
"cell_type": "code",
"execution_count": 20,
"metadata": {},
"outputs": [],
"source": [
"import warnings\n",
"warnings.filterwarnings(\"ignore\")"
]
},
{
"cell_type": "code",
"execution_count": 21,
"metadata": {
"colab": {
  "base_uri": "https://localhost:8080/",
  "height": 352
},
"id": "KYhtboJm_Iyx",
"outputId": "9ca1098c-9f0f-45d4-9087-da7c03b2b141"
},
"outputs": [],
"source": [
"from nltk.tag import tnt \n",
"list_word_tag_train_sentences = create_word_tag_tuples(ud_train)\n",
"list_word_tag_dev_sentences = create_word_tag_tuples(ud_dev)\n",
"list_word_tag_test_sentences = create_word_tag_tuples(ud_test)\n",
"\n",
"tnt_pos_tagger = tnt.TnT()\n",
"tnt_pos_tagger.train(list_word_tag_train_sentences)\n",
"\n",
"memm_tagger_word_accuracy_train = round(tnt_pos_tagger.evaluate(list_word_tag_train_sentences) * 100, 4)\n",
"memm_tagger_word_accuracy_test = round(tnt_pos_tagger.evaluate(list_word_tag_dev_sentences) * 100, 4)\n",
"memm_tagger_word_accuracy_dev = round(tnt_pos_tagger.evaluate(list_word_tag_test_sentences) * 100, 4)"
]
},
{
"cell_type": "code",
"execution_count": 22,
"metadata": {},
"outputs": [],
"source": [
"def evaluate_memm(data, tagger):\n",
"        sentences_list = create_word_tag_tuples(data)\n",
"\n",
"        sentences_success = 0\n",
"        idx = pd.IndexSlice\n",
"        for sentence in sentences_list:\n",
"            count_successes = 0\n",
"            predicted_tags = tagger.tag([word[0] for word in sentence])\n",
"            for (_, actual_tag), predicted_tag in zip(sentence, predicted_tags):\n",
"                if predicted_tag[1] == actual_tag:\n",
"                    count_successes += 1\n",
"\n",
"            if count_successes == len(sentence)-1:\n",
"                sentences_success += 1\n",
"\n",
"        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)\n",
"\n",
"        return sentence_accuracy"
]
},
{
"cell_type": "code",
"execution_count": 23,
"metadata": {},
"outputs": [],
"source": [
"memm_tagger_sentence_accuracy_train = evaluate_memm(ud_train, tnt_pos_tagger)\n",
"memm_tagger_sentence_accuracy_test = evaluate_memm(ud_test, tnt_pos_tagger)\n",
"memm_tagger_sentence_accuracy_dev = evaluate_memm(ud_dev, tnt_pos_tagger)"
]
},
{
"cell_type": "code",
"execution_count": 24,
"metadata": {},
"outputs": [
{
  "cell_type": "code",
  "execution_count": 24,
  "metadata": {},
  "outputs": [
    {
      "data": {
        "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>data-set</th>\n",
          "      <th>word-accuracy[%]</th>\n",
          "      <th>sentence-accuracy[%]</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>train</td>\n",
          "      <td>98.0858</td>\n",
          "      <td>73.6572</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>dev</td>\n",
          "      <td>83.3465</td>\n",
          "      <td>19.0985</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>test</td>\n",
          "      <td>86.0786</td>\n",
          "      <td>18.0089</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "</div>"
        ],
        "text/plain": [
          "  data-set  word-accuracy[%]  sentence-accuracy[%]\n",
          "0    train           98.0858               73.6572\n",
          "1      dev           83.3465               19.0985\n",
          "2     test           86.0786               18.0089"
        ]
      },
      "execution_count": 24,
      "metadata": {},
      "output_type": "execute_result"
    }
  ],
  "text/plain": [
    "  data-set  word-accuracy[%]  sentence-accuracy[%]\n",
    "0    train           98.0858               19.6290\n",
    "1      dev           83.3465               22.0641\n",
    "2     test           86.0786               19.7987"
  ]
},
"execution_count": 24,
"metadata": {},
"output_type": "execute_result"
}
],
"source": [
"result_list = [['train' ,memm_tagger_word_accuracy_train, memm_tagger_sentence_accuracy_train],\n",
"['dev', memm_tagger_word_accuracy_dev, memm_tagger_sentence_accuracy_dev],\n",
"['test', memm_tagger_word_accuracy_test, memm_tagger_sentence_accuracy_test]]\n",
"\n",
"memm_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
"memm_tagger_results.head()"
]
},
{
"cell_type": "markdown",
"metadata": {
"id": "5DIvvzsq_U-o"
},
"source": [
"## Print both, word level and sentence level accuracy for all the three taggers in a table."
]
},
{
"cell_type": "code",
"execution_count": 25,
"metadata": {
"id": "V32202cikh7u"
},
"outputs": [
{
"cell_type": "code",
"execution_count": 27,
"metadata": {
  "id": "V32202cikh7u"
},
"outputs": [
  {
    "data": {
      "text/html": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>tagger</th>\n",
        "      <th>data-set</th>\n",
        "      <th>word-accuracy[%]</th>\n",
        "      <th>sentence-accuracy[%]</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>memm</td>\n",
        "      <td>dev</td>\n",
        "      <td>83.3465</td>\n",
        "      <td>19.0985</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>memm</td>\n",
        "      <td>test</td>\n",
        "      <td>86.0786</td>\n",
        "      <td>18.0089</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>hmm</td>\n",
        "      <td>dev</td>\n",
        "      <td>83.5343</td>\n",
        "      <td>19.5730</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>hmm</td>\n",
        "      <td>test</td>\n",
        "      <td>79.2502</td>\n",
        "      <td>18.6801</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>simple tagger</td>\n",
        "      <td>dev</td>\n",
        "      <td>84.8731</td>\n",
        "      <td>16.3701</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>simple tagger</td>\n",
        "      <td>test</td>\n",
        "      <td>82.5907</td>\n",
        "      <td>16.2192</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
      ],
      "text/plain": [
        "          tagger data-set  word-accuracy[%]  sentence-accuracy[%]\n",
        "0           memm      dev           83.3465               19.0985\n",
        "1           memm     test           86.0786               18.0089\n",
        "2            hmm      dev           83.5343               19.5730\n",
        "3            hmm     test           79.2502               18.6801\n",
        "4  simple tagger      dev           84.8731               16.3701\n",
        "5  simple tagger     test           82.5907               16.2192"
      ]
    },
    "execution_count": 27,
    "metadata": {},
    "output_type": "execute_result"
  }
],
"source": [
  "result_list = [['memm', 'dev', memm_tagger_word_accuracy_dev, memm_tagger_sentence_accuracy_dev],\n",
  "['memm', 'test', memm_tagger_word_accuracy_test, memm_tagger_sentence_accuracy_test],\n",
  "['hmm', 'dev', hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev],\n",
  "['hmm', 'test', hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test],\n",
  "['simple tagger', 'dev', simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev],\n",
  "['simple tagger', 'test', simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test]]\n",
  "\n",
  "taggers_results =  pd.DataFrame(result_list, columns = ['tagger', 'data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
  "taggers_results.set_index('tagger')\n",
  "taggers_results"
]
},
"execution_count": 25,
"metadata": {},
"output_type": "execute_result"
}
],
"source": [
"result_list = [['memm', 'train' ,memm_tagger_word_accuracy_train, memm_tagger_sentence_accuracy_train],\n",
"['memm', 'dev', memm_tagger_word_accuracy_dev, memm_tagger_sentence_accuracy_dev],\n",
"['memm', 'test', memm_tagger_word_accuracy_test, memm_tagger_sentence_accuracy_test],\n",
"['hmm', 'train' ,hmm_word_level_accuracy_train, hmm_sentence_level_accuracy_train],\n",
"['hmm', 'dev', hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev],\n",
"['hmm', 'test', hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test],\n",
"['simple tagger', 'train' ,simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train],\n",
"['simple tagger', 'dev', simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev],\n",
"['simple tagger', 'test', simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test]]\n",
"\n",
"taggers_results =  pd.DataFrame(result_list, columns = ['tagger', 'data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])\n",
"taggers_results.set_index('tagger')\n",
"taggers_results"
]
}
],
"metadata": {
"colab": {
"collapsed_sections": [],
"name": "Assignment_2.ipynb",
"provenance": []
},
"kernelspec": {
"display_name": "Python 3 (ipykernel)",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.9.7"
}
},
"nbformat": 4,
"nbformat_minor": 4
}