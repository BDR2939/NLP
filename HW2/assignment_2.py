# -*- coding: utf-8 -*-
"""Assignment 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y2s0rdmuLTUbIcx6argONkluCRfZoaei

# Assignment 2

This assignment is about training and evaluating a POS tagger with some real data. The dataset is available through the Universal Dependencies (https://universaldependencies.org/) (UD) project. To get to know the project, please visit https://universaldependencies.org/introduction.html)
"""

import numpy as np
import  conllu 
from io import open
from conllu import parse_incr
from collections import defaultdict
import pandas as pd
import random


"""**Part 1** (getting the data)

You can download the dataset files directly from the UD website, but it will let you only download all the languages in one compressed file. In this assignment you will be working with th GUM dataset, which you can download directly from:
https://github.com/UniversalDependencies/UD_English-GUM.
Please download it to your colab machine.


"""

# !git clone https://github.com/UniversalDependencies/UD_English-GUM

"""We will use the (train/dev/test) files:

UD_English-GUM/en_gum-ud-train.conllu

UD_English-GUM/en_gum-ud-dev.conllu

UD_English-GUM/en_gum-ud-test.conllu

They are all formatted in the conllu format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.

You should write a code that reads the three datasets into memory. You may choose the data structure by yourself. As you can see, every word is represented by a line, with columns representing specific features. We are only interested in the first and fourth columns, corresponding to the word and its POS tag.
"""

# Your code goes here
user = 'Or'
# Your code goes here
user = 'Or'
if user == 'Or':
    ud_dev = r"C:\MSC\NLP2\HW2\en_gum-ud-dev.csv"
    ud_train = r"C:\MSC\NLP2\HW2\en_gum-ud-train.csv"
    ud_test = r"C:\MSC\NLP2\HW2\en_gum-ud-test.csv"
    
else:
    ud_dev = r"C:\MSC\NLP2\HW2\UD_English-GUM\en_gum-ud-dev.conllu"
    ud_train = r"C:\MSC\NLP2\HW2\UD_English-GUM\en_gum-ud-train.conllu"
    ud_test = r"C:\MSC\NLP2\HW2\UD_English-GUM\en_gum-ud-test.conllu"



def extract_ommision_matrix_B(train_df, unique_pos, unique_words):
    unique_words_list = unique_words.tolist()
    unique_pos_list = unique_pos.tolist()

    B = np.zeros([len(unique_pos), len(unique_words)])
    B_row_index = 0
    for i_pos in unique_pos:
        i_pos_train_df = train_df.loc[train_df['p'] == i_pos]
        i_pos_words, i_pos_word_count = np.unique(i_pos_train_df.loc[:, 'w'].values, return_counts=True)
        i_pos_percent = i_pos_word_count / np.sum(i_pos_word_count)
        i_pos_words_list = i_pos_words.tolist()
        for i_word in i_pos_words_list:
            if i_word in i_pos_words_list:
                word_index = i_pos_words_list.index(i_word)
                updated_percent_per_word_per_pos = i_pos_percent[word_index]
                B_column_word_index = unique_words_list.index(i_word)
                B[B_row_index, B_column_word_index] = updated_percent_per_word_per_pos
            else:
                continue
        B_row_index += 1

    return B


def generate_transition_matrix_A(train_df, unique_pos, unique_words):
    unique_words_list  = unique_words.tolist()
    unique_pos_list  = unique_pos.tolist()
    A = np.zeros([len(unique_pos), len(unique_pos)])
    rol_train_df = train_df.copy()
    rol_train_df = rol_train_df.iloc[np.arange(-1, len(rol_train_df) - 1)].reset_index(drop=True)
    rol_train_df = rol_train_df[1:]
    A_row_index = 0
    for i_pos in unique_pos:
        index_2_slice = (train_df['p'] == i_pos)
        index_2_slice = index_2_slice.iloc[np.arange(-1, len(index_2_slice) - 1)].reset_index(drop=True)
        index_2_slice.iloc[0] = False
        i_pos_train_df = train_df.loc[index_2_slice]

        i_pos_pos, i_pos_pos_count = np.unique(i_pos_train_df.loc[:, 'p'].values, return_counts=True)
        i_pos_percent = i_pos_pos_count / np.sum(i_pos_pos_count)
        i_pos_pos_list = i_pos_pos.tolist()
        for i_pos_next in i_pos_pos_list:
            if i_pos_next in i_pos_pos_list:
                pos_index = i_pos_pos_list.index(i_pos_next)
                updated_percent_per_pos_per_pos = i_pos_percent[pos_index]
                A_column_pos_index = unique_pos_list.index(i_pos_next)
                A[A_row_index, A_column_pos_index] = updated_percent_per_pos_per_pos
            else:
                continue
        A_row_index += 1
    return A





def get_list_of_sentences_tag_lists(df):
    word_index_array = df['i'].to_numpy()
    initial_sentence_idx = np.where(word_index_array == 1)[0]
    words_list = df['w'].to_list()
    tag_list = df['p'].to_list()

    sentence_list  = (initial_sentence_idx.size)*['None']
    sentence_tag_list  = (initial_sentence_idx.size)*['None']

    for sentence_idx in range(initial_sentence_idx.size):
        if not sentence_idx == initial_sentence_idx.size-1:
            
            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]
            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]:initial_sentence_idx[sentence_idx+1]]
        else:
            curr_sentence =  words_list[initial_sentence_idx[sentence_idx]::]
            curr_tag =  tag_list[initial_sentence_idx[sentence_idx]::]
        
        sentence_list[sentence_idx] = curr_sentence
        sentence_tag_list[sentence_idx] = curr_tag

    return sentence_list, sentence_tag_list


def sentences_from_df(df):
    sentences = []
    sentence_ind = 1
    sentence = []
    
    
        
    for i in range(df.shape[0]):
        if not 's' in df.columns:
            curr_sentence_ind = df.index[i][0]
        else:
            curr_sentence_ind = df.iloc[i]['s']

        if curr_sentence_ind != sentence_ind:
            sentences.append(str.join(" ", sentence))
            sentence_ind += 1
            sentence = []
        sentence.append(df.iloc[i, :]['w'])

    return sentences



class simple_tagger:
    def __init__(self):
        self.tagger = {}
    
    def train(self, data):
        pos = np.unique(data['p'], return_counts=True)
        most_frequent_pos = pos[0][np.where(pos[1] == max(pos[1]))][0]

        tagger = defaultdict(lambda: most_frequent_pos)

        unique_words = np.unique(data.loc[:, 'w'].values)
        for word in unique_words:
            word_data = data.loc[data['w'] == word]
            word_data_count = word_data['p'].value_counts()
            predicted_tag = word_data_count.index[0]
            # predicted_tag = pos_of_word = np.unique(data.loc[data['w'] == word, 'p'], return_counts=True)
            tagger[word] = predicted_tag

        self.tagger = tagger
    def evaluate(self, data):
        # sentences = sentences_from_df(data)
        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)

        words_success = 0
        sentences_success = 0
        idx = pd.IndexSlice
        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):
        # for sentence_num, sentence in enumerate(sentences, 1):
            count_successes = 0
            # sentence_df = data.loc[idx[sentence_num], 'p'].values
            for word_num, (word, actual_tag) in enumerate(zip(sentence, sentence_tags)):
            # for word_num, word in enumerate(sentence.split(' ')):
                predicted_tag = self.tagger[word]
                # actual_tag = sentence_df[word_num]

                if predicted_tag == actual_tag:
                    words_success += 1
                    count_successes += 1

            if count_successes == len(sentence)-1:
                sentences_success += 1

        word_accuracy = round(words_success / data.shape[0] * 100, 4)
        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)

        return word_accuracy, sentence_accuracy
    
    
    




train_df = pd.read_csv(ud_train)
dev_df = pd.read_csv(ud_dev)
test_df = pd.read_csv(ud_test)




# sentences1 = sentences_from_df(train_df)

# sentences, tag = get_list_of_sentences_tag_lists(train_df)

# tagger = simple_tagger()
# tagger.train(train_df)


# simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train = tagger.evaluate(train_df)
# simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev = tagger.evaluate(dev_df)
# simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test = tagger.evaluate(test_df)


# result_list = [['train' ,simple_tagger_word_accuracy_train, simple_tagger_sentence_accuracy_train],
# ['dev', simple_tagger_word_accuracy_dev, simple_tagger_sentence_accuracy_dev],
# ['test', simple_tagger_word_accuracy_test, simple_tagger_sentence_accuracy_test]]

# simple_tagger_results =  pd.DataFrame(result_list, columns = ['data-set', 'word-accuracy[%]', 'sentence-accuracy[%]'])
# simple_tagger_results.head()
# print(f'train data: word accuracy = {simple_tagger_word_accuracy_train}%, sentence accuracy = {simple_tagger_sentence_accuracy_train}%')
# print(f'dev data: word accuracy = {simple_tagger_word_accuracy_dev}%, sentence accuracy = {simple_tagger_sentence_accuracy_dev}%')
# print(f'test data: word accuracy = {simple_tagger_word_accuracy_test}%, sentence accuracy = {simple_tagger_sentence_accuracy_test}%')


a=5 
#     pass
# import operator
# import nltk


# ______________________________________________________________________________________________________________________________________
"""**Part 2**

Write a class **simple_tagger**, with methods *train* and *evaluate*. The method *train* receives the data as a list of sentences, and use it for training the tagger. In this case, it should learn a simple dictionary that maps words to tags, defined as the most frequent tag for every word (in case there is more than one most frequent tag, you may select one of them randomly). The dictionary should be stored as a class member for evaluation.

The method *evaluate* receives the data as a list of sentences, and use it to evaluate the tagger performance. Specifically, you should calculate the word and sentence level accuracy.
The evaluation process is simply going word by word, querying the dictionary (created by the train method) for each word’s tag and compare it to the true tag of that word. The word-level accuracy is the number of successes divided by the number of words. For OOV (out of vocabulary, or unknown) words, the tagger should assign the most frequent tag in the entire training set (i.e., the mode). The function should return the two numbers: word level accuracy and sentence level accuracy.

"""

# tagger = simple_tagger()
# tagger.train(train_df)

# # https://piazza.com/class/klxc3m1tzqz2o8?cid=40 - "You should evaluate on the test and dev datasets separately. The train file is for training only"
# simple_tagger_word_level_accuracy_train, simple_tagger_sentence_level_accuracy_train = tagger.evaluate(train_df)
# simple_tagger_word_level_accuracy_test, simple_tagger_sentence_level_accuracy_test = tagger.evaluate(test_df)
# simple_tagger_word_level_accuracy_dev, simple_tagger_sentence_level_accuracy_dev = tagger.evaluate(dev_df)


# print(f'*train* data: word accuracy = {simple_tagger_word_level_accuracy_train} %, sentence accuracy = {simple_tagger_sentence_level_accuracy_train} %')
# print(f'*test* data: word accuracy = {simple_tagger_word_level_accuracy_test} %, sentence accuracy = {simple_tagger_sentence_level_accuracy_test} %')
# print(f'*dev* data: word accuracy = {simple_tagger_word_level_accuracy_dev} %, sentence accuracy = {simple_tagger_sentence_level_accuracy_dev} %')


"""**Part 3**

Similar to part 2, write the class hmm_tagger, which implements HMM tagging. The method *train* should build the matrices A, B and Pi, from the data as discussed in class. The method *evaluate* should find the best tag sequence for every input sentence using he Viterbi decoding algorithm, and then calculate the word and sentence level accuracy using the gold-standard tags. You should implement the Viterbi algorithm in the next block and call it from your class.

Additional guidance:
1. The matrix B represents the emissions probabilities. Since B is a matrix, you should build a dictionary that maps every unique word in the corpus to a serial numeric id (starting with 0). This way columns in B represents word ids.
2. During the evaluation, you should first convert each word into it’s index and then create the observation array to be given to Viterbi, as a list of ids. OOV words should be assigned with a random tag. To make sure Viterbi works appropriately, you can simply break the sentence into multiple segments every time you see an OOV word, and decode every segment individually using Viterbi.

"""


### Create matrices
pos_values = list(np.unique(train_df.loc[:, 'p'].values, return_counts=True))
unique_words = np.unique(train_df.loc[:, 'w'].values)
unique_pos= pos_values[0]
unique_words_list  = unique_words.tolist()
unique_pos_list  = unique_pos.tolist()


# B = extract_ommision_matrix_B(train_df, unique_pos, unique_words)
# A = generate_transition_matrix_A(train_df, unique_pos, unique_words)
# pi_initial_matrix = np.ones([unique_pos.size,1])*(1/unique_pos.size)

# get_list_of_sentences(train_df)

POS = 'p'
WORD = 'w'
def get_list_of_sentences(df):
    number_of_sentences = df['w'].keys()[-1][0] # get the last sentence (s column) value === number of sentences in df
    sentences = [df.loc[(i)]['w'].str.cat(sep=' ') for i in range(1, number_of_sentences + 1)]
    return sentences



def wordsToIdx(words, B_columns):
    columns_as_list = list(B_columns)
    wordsAsIdxSegment = []
    
    for word in words:
        wordsAsIdxSegment.append(columns_as_list.index(word))

    return wordsAsIdxSegment


def segmentOfWordsToSentencePrediction(self, segment, sentencePredict, last=False):
    if segment:
        segmentIdx = wordsToIdx(segment, self.B.columns)
        segmentPredict = viterbi(segmentIdx, self.A, self.B, self.Pi)
        sentencePredict += segmentPredict

    if not last:
        random_tag = np.random.choice(self.A.index)
        sentencePredict.append(random_tag)

def update_A(A, i, currentSentenceTags):
    currentTag = currentSentenceTags[i]
    previousTag = currentSentenceTags[i-1]
    A.loc[previousTag, currentTag] +=1


def update_B(B, currentSentenceWords, currentSentenceTags):
    for word, tag in zip(currentSentenceWords, currentSentenceTags):
        B.loc[tag, word]+=1


def updatePi(Pi, currentTag):
    Pi[currentTag] +=1
    
    
def calc_A_B_Pi(data):
      POS = 'p'
      WORD = 'w'
      number_of_sentences = data.iloc[-1]['s']
      uniqueTags = set(data[POS])
      uniqueWords = set(data[WORD])

      # numOfTagsOccuresDict = dict.fromkeys(list(uniqueTags),0)
      A = pd.DataFrame(index=uniqueTags, columns=uniqueTags).fillna(0)
      B = pd.DataFrame(index=uniqueTags, columns=uniqueWords).fillna(0)

      # dict for every tag and its initial probability
      Pi = dict.fromkeys(list(uniqueTags),0)

      for j in range(1, number_of_sentences+1):
        currentSentenceTags = data.loc[(j)][POS]
        currentSentenceWords = data.loc[(j)][WORD]

        update_B(B, currentSentenceWords, currentSentenceTags)

        for i in range(1, len(currentSentenceTags)+1):

          if i == 1:
            currentTag = currentSentenceTags[i]
            updatePi(Pi, currentTag)
          else:
            update_A(A, i, currentSentenceTags)

      A["sum"] = A.sum(axis=1)
      A = A.loc[:, A.columns != 'sum'].div(A["sum"], axis=0)

      B["sum"] = B.sum(axis=1)
      B = B.loc[:, B.columns != 'sum'].div(B["sum"], axis=0)

      for tag in Pi.keys():
         Pi[tag] /= number_of_sentences

      return A, B, Pi
def dptable(V):

   yield " ".join(("%12d" % i) for i in range(len(V)))
   for state in V[0]:
       yield "%.7s: " % state + " ".join("%.7s" % ("%f" % v[state]["prob"]) for v in V)

def viterbi_algorithm(observations, states, start_p, trans_p, emit_p):
    
    
       
    
                    
                    
     V = [{}]
     for st in states:
         V[0][st] = {"prob": start_p[st] * emit_p[st][observations[0]], "prev": None}
   
     for t in range(1, len(observations)):
         V.append({})
         for st in states:
            max_tr_prob = V[t - 1][states[0]]["prob"] * trans_p[states[0]][st]
            prev_st_selected = states[0]
            for prev_st in states[1:]:
                tr_prob = V[t - 1][prev_st]["prob"] * trans_p[prev_st][st]
                if tr_prob > max_tr_prob:
                    max_tr_prob = tr_prob
                    prev_st_selected = prev_st
 
            max_prob = max_tr_prob * emit_p[st][observations[t]]
            V[t][st] = {"prob": max_prob, "prev": prev_st_selected}
     
     # for line in dptable(V):
     #    print(line)
 
     opt = []
     max_prob = 0.0
     best_st = None
 
     for st, data in V[-1].items():
        if data["prob"] > max_prob:
            max_prob = data["prob"]
            best_st = st
     opt.append(best_st)
     previous = best_st
 
 
     for t in range(len(V) - 2, -1, -1):
        opt.insert(0, V[t + 1][previous]["prev"])
        previous = V[t + 1][previous]["prev"]
        
     return opt
 
    
 
class hmm_tagger:
    data = pd.DataFrame()
    A = []
    B = []
    Pi = []
    uniqueWordsArr = []
    
    def __init__(self, data_df):
        
        unique_pos, pos_counts = list(np.unique(data_df.loc[:, 'p'].values, return_counts=True))
        unique_words, word_counts = np.unique(data_df.loc[:, 'w'].values, return_counts=True)
        unique_words_list  = unique_words.tolist()
        unique_pos_list  = unique_pos.tolist()
        
        self.data_df = data_df
        self.unique_pos = unique_pos
        self.unique_words = unique_words
        self.unique_words_list = unique_words_list
        self.unique_pos_list = unique_pos_list


    
    def train(self):

        data_df = self.data_df
        unique_pos = self.unique_pos
        unique_words = self.unique_words
        
        
        B = extract_ommision_matrix_B(self.data_df, unique_pos, unique_words)
        A = generate_transition_matrix_A(self.data_df, unique_pos, unique_words)
        Pi = np.ones([unique_pos.size,1])*(1/unique_pos.size)

        B = pd.DataFrame(B.T, index = unique_words, columns = unique_pos)
        A = pd.DataFrame(A.T, index =unique_pos , columns = unique_pos)
        Pi = pd.DataFrame(Pi.T, columns = unique_pos)
   
        self.states = tuple(unique_pos)
        self.A = A.to_dict()
        self.B = B.to_dict()
        self.Pi = Pi.to_dict('records')[0]
        
        # self.A, self.B, self.Pi = calc_A_B_Pi(data_df)
        self.uniqueWordsArr = unique_words.tolist()

   
    
    def evaluate(self, data):
        # sentences = sentences_from_df(data)
        sentences_list, sentence_tag_list = get_list_of_sentences_tag_lists(data)
        sentenceIdx = [] 
        words_success = 0
        sentences_success = 0
        idx = pd.IndexSlice
        for sentence_num, (sentence, sentence_tags) in enumerate(zip(sentences_list, sentence_tag_list)):
            # for sentence_num, sentence in enumerate(sentences, 1):
            count_successes = 0
            
            
            # validate all word in sentence is in train data set otherwise
            # choose randomly word from bank of word
            for i_word_idx, word in enumerate(sentence):
                if not word in self.unique_words:
                    sentence[i_word_idx] = random.choice(self.unique_words_list)
                
                
            predicted_tag_list = viterbi_algorithm(tuple(sentence), self.states, self.Pi, self.A, self.B)  
            
            for word_num, (predicted_tag, actual_tag) in enumerate(zip(predicted_tag_list, sentence_tags)):
                # print('word nun ' + str(word_num))

                if predicted_tag == actual_tag:
                    words_success += 1
                    count_successes += 1

            if count_successes == len(sentence)-1:
                sentences_success += 1

        word_accuracy = round(words_success / data.shape[0] * 100, 4)
        sentence_accuracy = round(sentences_success / len(sentences_list) * 100, 4)

        return word_accuracy, sentence_accuracy






# # Viterbi
# def viterbi (observations, A, B, Pi):
#     #...
#     # creates variables
#     delta_df = pd.DataFrame(index=B.index, columns=np.arange(len(observations))).fillna(0) 
#     psi_df = pd.DataFrame(index=B.index, columns=np.arange(len(observations))).fillna(0)
#     best_sequence = [0]*len(observations)

#     firstObserv = observations[0]
#     listOfWords = list(B.columns)
#     listOftags = list(B.index)

#     firstObservWord = listOfWords[firstObserv]

#     # initializtion step
#     for tag in delta_df.index:
#         b = B.loc[tag, firstObservWord]
#         delta_df.loc[tag, 0] = b*Pi[tag][0]
#         psi_df.loc[tag, 0] = 0

#     # iteration step
#     for observIdx, currentObserv in enumerate(observations[1:], 1):
#         currentObservWord = listOfWords[currentObserv]
#         valuesToTakeMax = []

#         maxValue = 0
#         bestTagForMaxValue = ''

#         for tag in delta_df.index:
#             b = B.loc[tag, currentObservWord]

#             for tag2 in delta_df.index:
#                 currentValue = delta_df.loc[tag2, observIdx-1] * A.loc[tag2, tag]

#             if currentValue > maxValue:
#                 maxValue = currentValue
#                 bestTagForMaxValue = tag2
#         delta_df.loc[tag, observIdx] = maxValue*b
#         psi_df.loc[tag, observIdx] = bestTagForMaxValue


#     # sequence recovery
#     t = len(observations)-1
#     best_sequence[t] = findBestTag(delta_df, t)
#     try:
#         for t in range(len(observations)-2, -1, -1):
#             best_sequence[t] = psi_df.loc[best_sequence[t], t+1]
#     except:
#         pass
    
#     return best_sequence

# A simple example to run the Viterbi algorithm:
#( Same as in presentation "NLP 3 - Tagging" on slide 35)

# A = np.array([[0.3, 0.7], [0.2, 0.8]])
# B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])
# Pi = np.array([0.4, 0.6])
# print(viterbi([0, 3, 1, 0], A, B, Pi))
# Expected output: 1, 1, 1, 1


hmmTagger = hmm_tagger(train_df)
hmmTagger.train()
test_df = test_df[0:51]
hmm_word_level_accuracy_test, hmm_sentence_level_accuracy_test = hmmTagger.evaluate(test_df)
# hmm_word_level_accuracy_dev, hmm_sentence_level_accuracy_dev = hmmTagger.evaluate(dev_df)

# print(f'*test* data: word accuracy = {hmm_word_level_accuracy_test} %, sentence accuracy = {hmm_sentence_level_accuracy_test} %')
# print(f'*dev* data: word accuracy = {hmm_word_level_accuracy_dev} %, sentence accuracy = {hmm_sentence_level_accuracy_dev} %')



# A simple example to run the Viterbi algorithm:
#( Same as in presentation "NLP 3 - Tagging" on slide 35)

# A = np.array([[0.3, 0.7], [0.2, 0.8]])
# B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])
# Pi = np.array([0.4, 0.6])
# print(viterbi([0, 3, 1, 0], A, B, Pi))
# Expected output: 1, 1, 1, 1

# A simple example to run the Viterbi algorithm:
#( Same as in presentation "NLP 3 - Tagging" on slide 35)

# A = np.array([[0.3, 0.7], [0.2, 0.8]])
# B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])
# Pi = np.array([0.4, 0.6])
# print(viterbi([0, 3, 2, 0], A, B, Pi))
# Expected output: 1, 1, 1, 1

"""**Part 4**

Compare the results obtained from both taggers and a MEMM tagger, implemented by NLTK (a known NLP library), over both, the dev and test datasets. To train the NLTK MEMM tagger you should execute the following lines (it may take some time to train...):
"""

from nltk.tag import tnt 

# tnt_pos_tagger = tnt.TnT()
# tnt_pos_tagger.train(train_data)
# print(tnt_pos_tagger.evaluate(test_data))

"""Print both, word level and sentence level accuracy for all the three taggers in a table."""

# Your code goes here

observations = ("normal", "cold", "dizzy")
states = ("Healthy", "Fever")
start_p = {"Healthy": 0.6, "Fever": 0.4}
trans_p = {
    "Healthy": {"Healthy": 0.7, "Fever": 0.3},
    "Fever": {"Healthy": 0.4, "Fever": 0.6},
}
emit_p = {
    "Healthy": {"normal": 0.5, "cold": 0.4, "dizzy": 0.1},
    "Fever": {"normal": 0.1, "cold": 0.3, "dizzy": 0.6},
}


 
     # for t in range(len(V) - 2, -1, -1):
     #     opt.insert(0, V[t + 1][previous]["prev"])
     #     previous = V[t + 1][previous]["prev"]
  
     # print ("The steps of states are " + " ".join(opt) + " with highest probability of %s" % max_prob)
     # a=5
viterbi_algorithm(observations, states, start_p, trans_p, emit_p)    





